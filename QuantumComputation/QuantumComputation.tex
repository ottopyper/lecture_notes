\documentclass[]{article}

\input{/home/pyotopic/Documents/PartIII/lazyboi/lecture_notes/preamble.tex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{xcolor}
\usepackage{pagecolor}
\usepackage[margin=1.2in]{geometry}
\usepackage{enumerate}
\usepackage{enumitem}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{mathtools}
%\DeclarePairedDelimiter\bra{\langle}{\rvert}
%\DeclarePairedDelimiter\ket{\lvert}{\rangle}
%\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1 \delimsize\vert #2}

\definecolor{thmcolour}{rgb}{0,0,0}
\definecolor{defcolour}{rgb}{0,0,0}
\definecolor{textcolour}{rgb}{0,0,0}
\definecolor{backgroundcolour}{rgb}{1,1,1}

\pagecolor{backgroundcolour}
\color{textcolour}

% \newtheoremstyle{custhm}
% {%space above
% 	1em
% }{%space below
% 	1em
% }{%body font
% 	\color{thmcolour}\itshape
% }{%indent amount
% 	-0em
% }{%head font
% 	\bfseries\color{thmcolour}
% }{%head punct
% }{%after head space
% 	1em
% }{%head spec
% \thmname{#1}\if\relax\detokenize{#2}\relax: \else\thmnumber{ #2}: \fi\if\relax\detokenize{#3}\relax\else\thmnote{(#3)}\fi
% }

% \newtheoremstyle{remark}
% {%space above
% }{%space below
% }{% body font
% }{%indent amount	-0em
% }{%head font
% \bfseries
% }{%head punct
% }{%after head space
% 0em
% }{%head spec
% \if\relax\detokenize{#3}\relax\thmname{#1}: \else\thmname{#3}: \fi
% }

% \newtheoremstyle{cusdef}
% {%space above
% 	1em
% }{%space below
% 	1em
% }{%body font
% 	\color{defcolour}
% }{%indent amount
% 	-0em
% }{%head font
% 	\bfseries\color{defcolour}
% }{%head punct
% }{%after head space
% 	1em
% }{%head spec
% 	%if numbered, include number
% 	%if named, include name
% 	\thmname{#1}
% 	\if\relax\detokenize{#2}\relax:
% 	\else\thmnumber{ #2}:\fi
% 	\if\relax\detokenize{#3}\relax
% 	\else\thmnote{ (#3)}\fi
% }

% \theoremstyle{custhm}
% \newtheorem{theorem}{Theorem}[section]
% \theoremstyle{cusdef}
% \newtheorem{defin}[theorem]{Definition}
% \theoremstyle{custhm}
% \newtheorem{lemma}[theorem]{Lemma}
% \theoremstyle{custhm}
% \newtheorem{cor}[theorem]{Corollary}

% \theoremstyle{custhm}
% \newtheorem{prop}[theorem]{Proposition}

% \theoremstyle{custhm}
% \newtheorem*{theorem*}{Theorem}

% \theoremstyle{cusdef}
% \newtheorem*{defin*}{Definition}

% \theoremstyle{remark}
% \newtheorem*{remark*}{Remark}


%\marginpar{to describe which lecture it is}

%\newcommand{\N}{\mathbb{N}}
%\newcommand{\Z}{\mathbb{Z}}
%\newcommand{\Q}{\mathbb{Q}}
%\newcommand{\R}{\mathbb{R}}
%\newcommand{\C}{\mathbb{C}}
%\newcommand{\e}{\mathrm{e}}
%\newcommand{\ra}{\rightarrow}
%\newcommand{\lef}{\left(}
%\newcommand{\res}{\right)}
%\newcommand{\ie}{\textit{i.e. }}
%\newcommand{\eps}{\varepsilon}
%\newcommand{\E}{\mathbb{E}}
%\newcommand{\suminf}{\sum_{n=0}^{\infty}}
%\newcommand{\suminfa}[1]{\sum_{#1=0}^{\infty}}
%\renewcommand{\P}{\mathbb{P}}
%\newcommand{\undf}[1]{\textit{\textbf{#1}}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\it}[1]{\textit{#1}}
%\newcommand{\M}{\mathcal{M}}
\renewcommand{\phi}{\varphi}
%\newcommand{\proves}{\vdash}
%\newcommand{\lra}{\leftrightarrow}
\renewcommand{\value}{|\cdot|}
\newcommand{\val}[1]{\left|#1\right|}
\newcommand{\valk}{(K,|\cdot|)}
\renewcommand{\bar}{\overline}
\renewcommand{\O}{\mathcal{O}}
%\newcommand{\A}{\mathcal{A}}
\newcommand{\qft}{\textrm{QFT}}

\newcommand{\poly}{\textrm{poly}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\QFT}{\textrm{QFT}}
\newcommand{\per}{\ket{\textrm{per}}}


\renewcommand{\lnot}{\neg}
\newcommand{\false}{\bot}
\newcommand{\true}{\top}
%opening
\title{Quantum Computation}
\author{Lectures by Richard Jozsa}
\date{}

\begin{document}

\maketitle

\tableofcontents
\clearpage
\marginpar{Lecture 1}
\section{Review of Shor's Algorithm}

This result is powered by the \textbf{quantum period finding algorithm}, and will lead us to the \textbf{hidden subgroup problem} (henceforce HSP).

\subsection{Factoring Problem}

Given an integer $N$, with $n = O(\log N)$ digits, we want to find a non-trivial factor in time complexity $O(\textrm{poly}(n))$.

The important concept here is that of \textbf{polynomial time complexity}: any computation has an input, from which we obtain an input \textit{size} $n$. Then by polynomial time complexity, we mean that the number of steps/gates (either classical or quantum) grows only polynomially with $n$ (\textit{i.e.} is $O(\textrm{poly}(n))$).

When we refer to \textbf{efficient} computation, we are always referring to polynomial time complexity.

The best known \textit{classical} factoring algorithm has complexity $\e^{O\lef n^{\frac{1}{3}}(\log n)^{\frac{2}{3}}\res}$. However, the best known quantum algorithm (due to Shor) runs in $O(n^3)$, a considerable improvement.

\subsection{Quantum Factoring Algorithm Summary}

First, we convert factoring into period determination:

Given $N$, choose $a<N$ with $(a,N) = 1$ and consider $f:\Z \ra \Z_N,\ x\mapsto a^x \mod N$. Euler's Theorem tells us that $f$ is periodic, and the period $r$ is the order of $a$ modulo $N$, \ie the least $m > 1$ such that $a^m \equiv 1 \mod N$ - this exists if and only if $a,N$ are coprime. Through knowledge of $r$ we are able to compute a factor of $N$.

While the process of determining $r$ is \textit{mathematically} very simple, it is in fact as difficult to compute from a classical perspective as factoring $N$ itself. Instead we use the \textbf{Quantum algorithm for periodicity determination}.

\textbf{\underline{The task}:} Given an oracle/black box for $f:\Z_M \ra \Z_N$ with promises:
\begin{itemize}
	\item $f$ is periodic, with (unknown) period $r\in \Z_M$, \ie f(x+r) = f(x) for all $x\in \Z_M$.
	\item $f$ is $1-1$ in each period, \ie $f(x_1)\ne f(x_2)$ for any $0\le x_1 < x_2 < r$.
\end{itemize}

We want to find $r$ in time $O(\poly(m)),\ m = \log M$ (with any prescribed success probability $1-\eps,\ \eps > 0$).

\begin{remark*}
	Queries to the oracle count as 1 step. In the quantum context we assume the oracle is a unitary gate $\U_f$ on $\U_M\otimes \U_N$, where $\U_M$ is the state space with dimension $M$, basis $\{ \ket{i} \}_{i\in\Z_M}$. $U_f$ acts on basis states as
	\[
	U_f\underbrace{\ket{i}}_{\textrm{\tiny input}}\underbrace{\ket{j}}_{\textrm{\tiny output}} = \ket{i}\ket{j+f(i)},\qquad i\in\Z_M,\ j\in\Z_M
	\]
	
	The \textbf{Query complexity} of an algorithm is the number of times the oracle is queried, which is also required to be $O(\poly(m))$.
\end{remark*}

To solve the periodicity problem classically, it can be shown that it is both necessary and sufficient to query the oracle $O(\sqrt{N})$ times, so there is no polynomial algorithm. However, there \textit{is} a quantum algorithm.

\subsection{Quantum Algorithm for Periodicity Determination}

For further details \textit{c.f.} Part II notes pp.60-64.

Write $A = M/r = \#$periods. We work in the state space $\U_M\otimes\U_N$ with basis $\{\ket{i}\ket{k}:\ i\in\Z_M,\ k\in\Z_N\}$.

\underline{\textbf{Step 1}}: obtain the state \[ \frac{1}{\sqrt{M}} \sum_{i=0}^{M-1}\ket{i}\ket{0}\]

\underline{\textbf{Step 2}}: apply $U_f$ to obtain \[\frac{1}{\sqrt{M}}\sum_{i=0}^{M-1}\ket{i}\ket{f(i)}\]

\underline{\textbf{Step 3}}: measure the output register, obtaining result $y$. By the \textbf{Born rule}, the input register collapses to all those $i$ such that $f(i) = y$, \ie $i = x_0,\ x_0+r,\cdots,\ x_0+(A-1)r$ where $0\le x_0 < r$ in the first period has $f(x_0) = y$.

We discard the output reigister to obtain \[\ket{\textrm{per}} = \frac{1}{\sqrt{A}}\sum_{j=0}^{A-1}\ket{x_0+jr}\]

Note that each $0\le x_0 < r$ occurs with probability $1/r$.

If we naively measure $\ket{\textrm{per}}$, the Born rule implies we get $x_0 + jr$ with $j = 0,\cdots,A-1$ chosen uniformly with probability $1/A$, \ie a random element of a random period; this is a uniformly random integer in $\Z_M$. This is useless to us. Instead...

\underline{\textbf{Step 4}}: apply \textbf{Quantum Fourier Transform} (QFT).
\marginpar{Lecture 2}
Recall that
\[
\textrm{QFT}\ket{x} = \frac{1}{\sqrt{M}} \sum_{y=0}^{M-1} \omega^{xy} \ket{y}
\]
\begin{remark*}[Fact]
QFT modulo $M$ is unitary, and can be implemented in $O(m^2)$ time, $m = \log M$. See Part II QIC notes for circuit details of implementation.
\end{remark*}

Then
\begin{align*}
	\QFT\ket{\textrm{per}} &= \frac{1}{\sqrt{MA}}\sum_{j=0}^{A-1} \left(\sum_{j=0}^{M-1}\omega^{(x_0+jr)y}\ket{y}\right)\\
	&=\frac{1}{\sqrt{MA}}\sum_{y=0}^{M-1}\omega^{x_0y}\left[\sum_{j=0}^{A-1}\omega^{jry}\right]\ket{y}
\end{align*}
Note that $[\cdots]$ is a geometric series, with ratio $\omega^{ry} = \e^{2\pi iry/M} = \left(\e^{e\pi i/A}\right)^y$. So the sum equals zero unless $y$ is a multiple of $A = M/r$, in which case it every term in the sum is 1 so the sum equals $A$. So the non-multiples of $A$ get sifted out by QFT.

Hence, we have
\begin{align*}
\QFT\ket{\textrm{per}} = \sqrt{\frac{A}{M}}\sum_{k=0}^{r-1}\omega^{x_0kM/r}\ket{k\frac{M}{r}}
\end{align*}
Then measuring $\QFT\ket{\textrm{per}}$ we get a value $c = k_0 M/r$, with $0\le k_0\le r-1$ chosen uniformly at random. Thus we have $k_0/r = c/M$, where the values $c,M$ are known and $k_0$ has been chosen at random; we want $r$. Note that if we are fortunate enough to have $(k_0,r) = 1$, then we can (efficiently) cancel $c/M$ down to its lowest terms, and read off $r$ as the denominator. But in general this will not be the case:

\begin{theorem*}[Coprimality Theorem]
The number of positive integers $<r$ that are coprime to $r$ grows as $O\left(r/\log\log r\right)$ for large $r$.
\end{theorem*}
Hence the above $\P(k_0 \textrm{ coprime to }r = O\left(1/\log\log r\right)$. So if we do it enough times, we will almost surely be successful:
	
\begin{remark*}[Probability Lemma]
If a single trial has success probability $p$, then we repeat $k$ times, and for any $0 < 1 - \eps < 1$, we have that
\begin{align*}
	\textrm{if }&\quad k= -\frac{\log \eps}{p}\\
	\textrm{then }&\quad\P(\ge 1 \textrm{ success in }k \textrm{ trials}) > 1 - \eps
\end{align*}
\end{remark*}
So after finding $c$, cancel $c/M$ down to its lowest terms $a/b$ (classically, in polynomial time using Euclid's algorithm). We get $r$ as denominator $b$ if $(k_0,r) = 1$, which happens with probability $O(1/\log\log r)$, otherwise $c,M$ have more common factors, so $b < r$.

We don't know immediately whether that has happened or not, but we can check the $b$ value by making two more queries to the oracle, $f(0)$ and $f(b)$; these are equal iff $b = r$.

So if we repeat this $K = O(\log\log r)$ times, then we will obtain $r$ with any high probability we desire - and this runs in polynomial time.

\undf{Origin and utility of QFT here}

Write $R = \{0,r,2r,\dots,(A-1)r\}\subset \Z_M$, and
\begin{align*}
\ket{R} &= \frac{1}{\sqrt{A}}\sum_{k=0}^{A-1}\ket{kr}\\
\per = \ket{x_0+R} = \frac{1}{\sqrt{A}}\sum_{k=0}^{A-1}\ket{x_0+kr}
\end{align*}
The problem is that the $\ket{x_0+kr}$ terms are distributed randomly.

For each $x_0\in\Z_M$, consider the map $k\mapsto k+x_0$ on $\Z_M$; this is the 1-1 reversible map ``shift by $x_0$''.

This gives rise to a linear map $U(x_0)$ on $\U_M$, and $U(x_0):\ket{k}\ra\ket{k+x_0}$ is unitary, and $\ket{x_0+R} = U(x_0)\ket{R}$.

Since $(\Z_M,+)$ is an \it{abelian} group, these shift operators all commute, \it{i.e.} $U(x_0)U(x_1) = U(x_0+x_1) = U(x_1)U(x_0)$. So they have an orthonormal basis of common eigenvectors $\{\ket{\chi_k}\}_{k\in\Z_M}$, called the \it{shift-invariant} states. Note that they are not left entirely unchanged by the $U(x_0)$ operators, but they are shifted only by a constant phase factor, \textrm{i.e.} $U(x_0)\ket{\chi_k} = \omega(x_0,k)\ket{\chi_k}$ for all $x_0,k\in\Z_M$, and $|\omega(x_0,k)| = 1$.

Now consider $\ket{R}$ written in the $\chi$-basis
\begin{align*}
\ket{R} = \sum_{k=0}^{M-1}a_k\ket{\chi_k}
\end{align*}
where the amplitudes $a_k$ depend only on $r$, and not on $x_0$ (obviously). Then $\per = U(x_0)\ket{R} = \sum a_k\omega(x_0,k)\ket{\chi_k}$, and measurement in the $\chi$-basis has $\P(k) = |a_k\omega(x_0,k)|^2 = |a_k|^2$, independent of $x_0$, depending only on $r$. So we want to measure in this basis, but aren't allowed to do that (computationally) since the basis is too complicated.

So we introduce QFT as the unitary mapping that rotates the $\chi_k$-basis onto the standard basis $\ket{k}$, and follow this up by a standard basis measurement.

But what does this mapping look like, and where does it come from? We need the explicit form of the shift-invariant eigenstates:
\begin{align*}
	\ket{\chi_k} &= \frac{1}{\sqrt{M}}\sum_{\ell = 0}^{M-1}\e^{-2\pi ik\ell/M}\ket{\ell}\\
	\implies \U(x_0)\ket{\chi_k} &= \frac{1}{\sqrt{M}}\sum_{\ell = 0}^{M-1}\e^{-2\pi ik\ell/M}\ket{\ell + x_0}\\
	&= \frac{1}{\sqrt{M}}\sum_{\tilde{\ell} = 0}^{M-1}\e^{-2\pi i k(\tilde{\ell} - x_0)}\ket{\tilde{\ell}}\\
	&= \e^{2\pi i kx_0/M}\ket{\chi_k}
\end{align*}
So $\omega(x_0,k) = \e^{2\pi i k x_0/M}$.

The matrix of $\QFT^{-1}$ (mapping $\ket{k}$ to $\ket{\chi_k}$) has components of $\ket{\chi_k}$ as the $k^{\textrm{th}}$ column, so $[\QFT^{-1}]_{\ell k} = \frac{1}{\sqrt{M}}\e^{-2\pi i\ell k/M}$. Since QFT is unitary, to find the inverse we need only take the conjugate transpose.

Hence $[QFT]_{k\ell} = \frac{1}{\sqrt{M}}\e^{2\pi ik\ell/M}$, as previously defined.

This notion of QFT in fact occurs very naturally in group theory as the \it{discrete Fourier transform}. The fact that this QFT is unitary means that all the group theoretic results it relies on slot in perfectly, allowing us to make as much use of this as we want in a way that we are not able classically.

\marginpar{Lecture 3}

The following algorithm was inspired by a desire to generalise the successful technique of Shor's celebrated algorithm.

\section{The Hidden Subgroup Problem (HSP)}

Let $G$ be a finite group, of size $|G|$.

We are given an oracle $f : G\ra X$ (where $X$ is some set), and a promise that there is a subgroup $K < G$ such that
\begin{itemize}
	\item $f$ is constant on (left) cosets of $K$ in $G$
	\item $f$ is distinct on distinct cosets.
\end{itemize}
\begin{remark*}[Problem]
Determine the ``hidden subgroup'' $K$.

For instance, this might entail outputting a set of generators, or we might be happy enough with just sampling uniformly from the elements of $K$.

$\ast$ we want to solve this in time $O(\poly\log|G|)$, with any constant probability $1 - \eps$.
\end{remark*}
\begin{remark*}[Examples of problems that can be seen as HSPs]\ 
\begin{enumerate}[label = (\alph*)]
	\item \underline{Periodicity}. $f:\Z_M \ra X$ periodic, period $r$, bijective within periods.
	
	Then let $G = \Z_M$, $K = \{0,r,2r,\dots\} < G$, and the cosets are $x_0 + K = \{x_0,x_0+r,x_0+2r\dots\}$. It is then clear that $f$ is constant/distinct on the cosets in the desired way.
	
	\item \underline{Discrete Logarithms}: this was also solved by Shor in his original paper.
	
	Take $p$ prime, and consider $\Z_p^\ast$ the group of units modulo $p$ = $\{1,2,\dots,p-1\}$. We say that $g\in \Z_p^\ast$ is a \it{generator}, or a \it{primitive root modulo} $p$, if the powers of $g$ generate all of $\Z_p^\ast$.
	
	\underline{Fact}: generators always exist, \it{i.e.} $\Z_p^\ast$ is always cyclic. For instance, $2,3$ both generate $\Z_5^\ast$ - though $1,4$ do not.
	
	So any $x\in \Z^\ast_p$ can be written as $x = g^y$ for $y \in \Z_{p-1}$. We thus write $y = \log_g x$ for the \undf{discrete logarithm} of $x$, to base $g$.
	
	The discrete log problem is then: given a generator $g$ and $x\in \Z^\ast p$, compute $y = \log_g x$. This is very difficult classically, and it underpins public key cryptography.
	
	To express this as a hidden subgroup problem, consider $f:\Z_{p-1}\times\Z_{p-1}\ra\Z_p^\ast$ by $f(a,b) = g^ax^{-b} = g^{a-yb} \mod p$. Then (\it{check}) $f(a_1,b_1) = f(a_2,b_2)$ iff $(a_2,b_2) = (a_1,b_1)+\lambda (y,1),\ \lambda \in \Z_{p-1}$.
	
	So we let $G = \Z_{p-1}\times\Z_{p-1}$, and $K = \{\lambda(y,1):\lambda \in \Z_{p-1}\} < G$. Then $f$ is constant/distinct as appropriate on the cosets of $K$, and the generator $(y,1)$ of $K$ gives $y = \log_g x$.
	
	\item \underline{Graph Problems}.
	
	Consider a graph $A = (V,E)$, $|V| = n$. We stipulate that these are undirected, that there is at most one edge between any pair of vertices, and that the vertices are labelled by $[n] = \{1,2,\dots,n\}$. We also might be interested in the adjacency matrix $M_A$, the $n\times n$ matrix given by $[M_A]_{ij} = \mathbb{I}[\{i,j\}\in E]$, which is always symmetric for an undirected graph.
	
	The group that will be of interest to us is $P_n \coloneqq$ the permutation group of $[n]$. So $|P_n| = n! \sim \sqrt{2\pi n}(n/e)^n$ has $|P_n|\sim O(n\log n) < O(n^2)$, which is polynomial in the number of vertices. This is what we want for the running time of a graph algorithm.
	
	The subgroup of interest is $\textrm{Aut}(A)$, the \undf{automorphism group} of $A < P_n$ the set of permutations $\pi \in P_n$ such that for all $i,j$, $\{i,j\}\in E$ iff $\{\pi(i),\pi(j)\}\in E$. What this means is that after permuting the labels of the graph, we are left with the same labelled graph.
	
	An associated HSP (non-abelian $G$):
	
	Take $G = P_n$, and $X = $ the set of all labelled graphs on $n$ vertices (equivalent to the set of all symmetric $n\times n$ 0/1-matrices).
	
	For any $A\in X$, we consider $f_A : G\ra X$, with $f_A(\pi) = \pi(A)$, \it{i.e.} the graph $A$ with its vertex labels permuted by $\pi$. A little bit of thought shows that $K = \textrm{Aut}(A)$ is the hidden subgroup for this problem; $f_A$ is constant on the automorphisms of $A$, for instance.
	
	An important application of this is that if we can sample uniformly from $K$, then we can solve the \undf{Graph Isomorphism Problem} (GI), which has received a lot of attention in complexity theory in recent years.
	
	Two labelled graphs $A,B$ each on $n$ vertices are \undf{isomorphic} if there is a bijective map (permutation) on the labels $\pi :[n]\ra[n]$ such that for all $i,j\in [n]$, $\{i,j\}\in A$ iff $\{\pi(i),\pi(j)\}\in B$. In other words, $A$,$B$ are the same underlying graph (\it{i.e.} ignoring their labels they are indistinguishable). We write $A\cong B$.
	
	The GI Problem is then: given graphs $A$ and $B$, determine whether or not they are isomorphic. This has many useful applications; \it{e.g.} if you can see some proteins and their structure, you may want to be able to tell which proteins are actually the same.
	
	This can again be expressed as a non-abelian HSP, \it{c.f.} Sheet 1.
	
	There is no known polynomial time classical algorithm, and in fact there is no known polynomial time quantum algorithm either. The problem is in NP, but is \it{not} believed to be NP-complete. A problem is NP if it is, roughly speaking, difficult to solve but easy to validate that you have the right answer once you've solved it. A problem is NP-complete if you can rephrase any other NP problem as this one, and then solve it that way - so solving an NP-complete problem solves all NP problems; it is the hardest problem in NP.
	
	We currently do not believe that even quantum algorithms are able to solve NP-complete problems efficiently, so it is in some sense hopeless to try and work on these problems even from a quantum perspective. However, they can do NP-incomplete problems, so factoring and GI \it{etc}... are good candidates to attempt.
	
	Laslo Babai (2017) found a \it{quasi}-polynomial time \it{classical} algorithm for GI; it has runtime $n^{O((\log n)^2)}$. This is slower than polynomial time, but faster than exponential time. We have the following hierarchy:
	
	\begin{align*}
		\poly (n) < n^{O((\log n)^2)} < \textrm{exp}\\
		2^{O(\log n)} < 2^O((\log n)^3) < 2^{O(n)}
	\end{align*}

So in terms of exponents, these are linear/polynomial/exponential in $\log n$.
\item Another non-abelian example is the \undf{dihedral group}; there is a connection to the HSP `shortest vector in a lattice'. We are given $n$ linearly independent vectors in $\R^n$, and consider their lattice; the problem is to find the lattice point closest to the origin.
\end{enumerate}
\end{remark*}

\marginpar{Lecture 4}

What we {\it do} have is:

\section*{Quantum Algorithm for Finite Abelian HSP}

We write the group $(G,+)$, additively. We need a couple of components:

\subsection*{Construction of shift invariant states \& Fourier transform for $G$}
\begin{defin*}[Representation of $G$]
A group homomorphism $\chi :G\ra\C^\ast = (\C\backslash\{0\},\cdot)$ is called a \undf{representation} of $G$.

For abelian groups, any such map is called an \undf{irreducible representation} (henceforth {\it irrep}) of $G$.
\end{defin*}

These have the following properties:

\begin{theorem*}[Theorem A]\ 
\begin{enumerate}[label=(\roman*)]
	\item any value $\chi(g)$ is a $|G|^{\textrm{th}}$ root of unity (so $\chi : G\ra S^1$, the unit circle in $\C$)
	\item (Schur's Lemma/Orthogonality) If $\chi_i$ and $\chi_j$ are representations, then
	\begin{align*}
		\frac{1}{|G|} \sum_{g\in G} \chi_i(g)\overline{\chi_j(g)} = \delta_{ij}
	\end{align*}
	\item There are exactly $|G|$ different representations of $G$.
\end{enumerate}
\end{theorem*}

By (iii) we can label the $\chi$'s as $\chi_g: g\in G$.

\begin{remark*}[Example]
$\chi(g) = 1$ for all $g\in G$ is always an irrep, called the \undf{trivial} irrep; label it as $\chi_0$ for $0\in G$.

Then for any other irrep $\chi\ne \chi_0$, orthogonality to $\chi_0$ gives
\begin{align*}
	\sum_{g\in G}\chi(g) = 0
\end{align*}
\end{remark*}

We now introduce:

\begin{defin*}[Shift-Invariant States, Shift Operators]
Consider the state sapce $\mathcal{H}_G$, dimension $|G|$, basis $\ket{g}:g\in G$.

The \undf{shift operators} are
\begin{align*}
	U(k) : \ket{g} \mapsto \ket{g+k},\ \ k,g\in G
\end{align*}
These all commute, so we have a simultaneous eigenbasis:

For each $\chi_k$, $k\in G$, consider the state
\begin{align*}
\ket{\chi_k} = \frac{1}{\sqrt{|G|}}\sum_{g\in G}\overline{\chi_k(g)}\ket{g}
\end{align*}

By Theorem A (ii), these form an orthonormal basis, and it follow from the group homomorphism property of irreps that these are in fact the simultaneous eigenbasis, $U(g)\ket{\chi_k} = \chi_k(g)\ket{\chi_k}$. These are the \undf{shift-invariant states}.
\end{defin*}
\begin{proof}
We have that
\begin{align*}
U(g)\ket{\chi_k} &= \frac{1}{\sqrt{|G|}}\sum_{h\in G}\overline{\chi_k}(h)\ket{g+h}\\
&= \frac{1}{\sqrt{|G|}}\sum_{h'\in G}\overline{\chi_k}(h'-g)\ket{h'}\\
&= \frac{1}{\sqrt{|G|}}\chi_k(g)\sum_{h'\in G}\overline{\chi_k}(h')\ket{h'}\\
&= \chi_k(g)\ket{\chi_k}
\end{align*}
Which follows since $\chi_k(g^{-1}) = \overline{\chi_k}(g)$.
\end{proof}
\begin{defin*}[Fourier Transform QFT]
The unitary gate on $\mathcal{H}_G$ mapping $\ket{\chi_g}$ basis to the $\ket{g}$ basis is known as the \undf{Quantum Fourier Transform (QFT)} {\it i.e.}
\begin{align*}
\qft\ket{\chi_g} &= \ket{g},\ \forall g\in G\\
\qft^{-1}\ket{g} &= \ket{\chi_g}
\end{align*}
In particular, the $k^{\textrm{th}}$ column of $\qft^{-1}$ in $\ket{g}$-basis is given by components of $\ket{\chi_k}$, {\it i.e.}
\begin{align*}
	\left[\qft^{-1}\right]_{gk} = \frac{1}{\sqrt{|G|}}\overline{\chi_k}(g)\ \ \forall g,k\in G
\end{align*}
so $QFT$ (as the conjugate transpose) has the matrix
\begin{align*}
\left[\qft\right]_{kg} &= \frac{1}{\sqrt{|G|}}\chi_k(g)\\
\implies \qft\ket{g} &= \frac{1}{\sqrt{|G|}}\sum_{k\in G}\chi_k(g)\ket{k}
\end{align*}
\end{defin*}
\begin{remark*}[Examples] $G = \Z_M$
Check $\chi_a(b) = \e^{2\pi i ab/M}$ for all $a,b\in \Z_M$ satisfies (Hom), so we have irreps naturally labelled by $a\in \Z_M$, $\chi_0(b) = 1$ for all $b$, giving the `usual' $\qft_M$ for $\Z_M$.

Similarly for $G = \Z_{M_1}\times\dots\times\Z_{M_r}$, where we have
\begin{align*}
\left.\begin{array}{c} (a_1,\dots,a_r) = g_1 \\ (b_1,\dots,b_r) = g_2\end{array}\right\rbrace \in G\\
\chi_{g_1}(g_2) \coloneqq \e^{2\pi i\left( \frac{a_1b_1}{M_1}+\dots+\frac{a_rb_r}{M_r}  \right)}
\end{align*}
which satisfies (Hom), and we get
\begin{align*}
\qft_G = \qft_{M_1}\otimes \cdots \otimes \qft_{M_r}
\end{align*}
\end{remark*}

Importantly, the above is exhaustive due to the classification of all finite abelian groups, which states that they are all isomorphic to a direct product $G\cong \Z_{M_1}\otimes\cdots\otimes\Z_{M_r}$, and furthermore we can take all the $M_i$'s to be (not necessarily distinct) prime powers. 

\subsection*{Quantum Algorithm for Finite Abelian HSP}

We have $f: G\ra X$, hidden subgroup $K$, cosets $K = 0 + K, g_2 + K,\dots,g_m+ K$, where $m = |G|/|K|$ is the number of cosets.

This works on the state space $\mathcal{H}_{|G|}\otimes\mathcal{H}_{|X|}$, with basis $\ket{g}\ket{x}$ for $g\in G$, $x\in X$.

\begin{itemize}
	\item Make the initial state
	\begin{align*}
		\frac{1}{\sqrt{|G|}}\sum_{g\in G} \ket{g}\ket{0}
	\end{align*}
	\item Apply $\mathcal{U}_f$, to obtain
	\begin{align*}
		\frac{1}{\sqrt{|G|}}\sum_{g\in G}\ket{g}\ket{f(g_0)}
	\end{align*}
	\item Measure the second register to see a value $f(g_0)$. Then the first register will give a coset state:
	\begin{align*}
		\ket{g_0 + K} = \frac{1}{\sqrt{|K|}} \sum_{k\in K}\ket{g_0 + k} = U(g_0)\ket{K}
	\end{align*}
	Where we are writing $\ket{K} = \frac{1}{\sqrt{|K|}}\sum_{k\in K}\ket{k}$.
	
	Here the coset has been chosen uniformly at random from all $|G|/|K|$ cosets.
	
	\item Apply $\qft$ and measure, to obtain a result $g\in G$.
	
	How does the output $g$ relate to $K$? Observe that
	\begin{itemize}
		\item The output distribution of $g$ is {\it independent} of $g_0$, so it is the same as that obtained from $\qft\ket{K}$, ({\it i.e.} $g_0 = 0$).
		
		This is since if we write $\ket{K}$ in the shift-invariant basis basis $\ket{\chi_g}$, $\ket{K} = \sum_{g\in G}a_g \ket{\chi_g}$, then $\ket{g_0+K} = U(g_0)\ket{K} = \sum a_g\chi_g(g_0)\ket{\chi_g}$. But $\qft \ket{\chi_g} = \ket{g}$, so after $\qft$ we have that $\P[g] = |a_g\chi_g(g_0)|^2 = |a_g|^2$ as $|\chi_g(g_0)| = 1$. Hence we have independence of $g_0$.
		
		But what is the distribution?
		
		\item How does it relate to $K$?
		\marginpar{Lecture 5}
		
		Recall that $$\qft\ket{k} = \frac{1}{\sqrt{|G|}} \sum_{\ell \in G} \chi_{\ell}(k)\ket{\ell}$$
		
		In particular
		\begin{align*}
			\qft\ket{K} = \frac{1}{\sqrt{|G|}} \frac{1}{\sqrt{K}} \sum_{\ell \in G} \left[\sum_{k\in K}\chi_{\ell}(k)\right] \ket{\ell}
		\end{align*}
	where $[\cdots]$ invovles irreps $\chi_\ell$ of $G$ restricted to $K < G$, which are irreps of $K$. Hence
	\begin{align*}
		[\cdots] = \left\lbrace \begin{array}{cc} |K|&\quad \textrm{ if }\chi_{\ell}\textrm{ restricts to trivial irrep on }$K$ \\ 0&\quad \textrm{ otherwise}\end{array}  \right.
	\end{align*}
So the measurement gives a uniformly random choice of $\ell$, such that $\chi_{\ell}(k) = 1$ for all $k \in K$, giving information about $K$.

	For instance, if $K$ has generators $k_1,\dots,k_M$, where $M = O(\log |K|) = O(\log |G|)$ (this is true for all finite groups), then the output has $\chi_{\ell}(k_i) = 1$ for all $i$.
	
	It can be shown that if $O(\log |G|)$ such $\ell$s are chosen uniformly at random, then with probability $> 2/3$ they suffice to determine the generating set via the equation $\chi_{\ell}(k) = 1$.
	
	See Sheet 1 \# 2 for an example of this problem and calculation.
	\end{itemize}
\end{itemize}

A trivial but nonetheless useful fact about irreps of $G$ is that they restrict to irreps of $K$, and in fact we could have a trivial irrep of $G$ that restricts to the trivial irrep of $K$ - as we saw above.

\begin{remark*}[Example]\
If $G = \Z_{M_1}\times\cdots\times \Z_{M_q}$, we had for $\ell = (\ell_1,\dots,\ell_q), g\in (b_1,\dots,b_q)\in G$ irreps $$\chi_{\ell}(g) = \e^{2\pi i\sum_{j=1}^{q}\left(\frac{\ell_j b_j}{m_j}\right)}$$

So for $k = (k_1,\dots,k_q)\in K$, the equation $\chi_{\ell}(k) = 1$ becomes
\[
\frac{\ell_1 k_1}{m_1} +\dots + \frac{\ell_q k_q}{m_q} \equiv 0\mod 1
\]
- that is to say, it is an integer. This is a homogeneous, linear equation in $k = (k_1,\dots,k_q)$ and $O(\log |K|)$ independent such equations determine $K$ as the null space of the linear system.
\end{remark*}

\begin{remark*}[Remarks on HSP for {\it non}-abelian groups $G$]
Disclaimer: there is no known efficient algorithm for this problem.

As before, we can easily generate coset states
\[
\ket{g_0 K} = \frac{1}{\sqrt{|K|}} \sum_{k \in K}\ket{g_0 k}
\]
where the $g_0$ are chosen randomly. But there arise new problems with the QFT construction. In particular, there is no basis of shift invariant states because the shift operators do not commute.

So if we apply the Fourier transform to the above state, it no longer works. We can make partial progress though.

\textbf{Construction of non-abelian FT}

Suppose we have $d$-dimensional representations of $G$, which are group homomorphisms
\[
\chi : G \ra U(d)
\]
where the $U(d)$ are $d\times d$ unitary matrices. So $\chi(g_1 g_2) = \chi(g_1)\cdot \chi(g_2)$, where $\cdot$ denotes matrix multiplication.

We then say $\chi$ is \undf{irreducible} (so an irrep) if no subspace of $\C^d$ is left invariant by all matrices $\chi(g): g\in G$. In other words, we cannot simultaneously block diagonalise all the $\chi(g)$s by a basis change.

Looking back at the abelian case, we can consider the irreps as matrices also, but since all the matrices commute with each other they can all be simultaneously block diagonalised, and reduce to 1-dimensional representations.

A \undf{complete set} of irreps is a set $\chi_1,\dots,\chi_M$ such that any irrep is unitarily equivalent to one of them, by which we mean $\chi \equiv \chi'$ if $\chi' = V\chi V^{-1}$ for some $V \in U(d)$.

\begin{theorem*}[Non-Abelian Generalisation of Theorem A]
If $d_1,\dots,d_M$ are the dimensions of a complete set of irreps $\chi_1,\dots,\chi_M$, then
\begin{enumerate}[label = (\roman*)]
	\item $d_1^2 +\dots + d_m^2 = |G|$
	\item Write $\chi_{i,jk}(g)$ for the $(j,k)^{\textrm{th}}$ entry of matrix $\chi_i(g)$. Then we have (Schur's orthogonality)
	\[
	\sum_{g\in G} \chi_{i,jk}(g)\bar{\chi}_{i',j'k'}(g) = |G| \delta_{ii'}\delta_{jj'}\delta{kk'}
	\]
\end{enumerate}
\end{theorem*}
Hence the states
\[
\ket{\chi_{i,jk}} \equiv \frac{1}{\sqrt{|G|}} \sum_{g\in G}\bar{\chi}_{i,jk}(g)\ket{g}
\]
form an orthonormal basis.

$\qft$ on $G$ is then defined as the unitary map that rotates the $\{\ket{\chi_{i,jk}}\}$ basis into the standard basis $\{\ket{g}\}$.

But the $\ket{\chi_{i,jk}}$ are \undf{not} shift invariant for all $U(g_0)$s, so measurement of the coset state $\ket{g_0 K}$ in the $\ket{\chi}$ basis has output distribution that is \undf{not} independent of $g_0$.

However, a `partial' shift invariance survives. Instead of performing a complete measurement, we can do an incomplete measurement on only the $i$ labels and not the $j$.

We call this measurement $M_{\textrm{rep}}$ on $\ket{g_0 K}$ that distinguishes only irrep labels ($i$ values) and \undf{not} all $(i,j,k)$s, {\it i.e.} the measurement outcome $i$ is associated to the $d_i^2$-dimensional subspace spanned by $\{\ket{\chi_{i,jk}}\}_{j,k=1,\dots,d_i}$.

Then it can be shown that $\chi_i(g_1g_2) = \chi_i(g_1)\chi_i(g_2)$ implies that the output distribution of the $i$-values is independent of $g_0$. This gives direct, but incomplete information about $K$.

For instance, conjugate subgroups $K$ and $L = g_0 Kg_0^{-1}$ for some $g_0 \in G$ give the same output distribution of $i$s.


For efficient HSP algorithm (if we use $\qft$), we need $\qft$ to be efficiently implementable, {\it i.e.} in $\poly(\log|G|)$ times - this is unusual, and a very special thing to ask for.

This is true for abelian groups, and some non-abelian groups, such as $P_n$ - but even for this group there is still no known efficient hidden subgroup algorithm.

Consider $\Z_{2^n}$; we have a tower of subgroups $\{0\} < \Z_2 < \Z_4 <\dots < \Z_{2^n}$, and the fast Fourier transform on this group works recursively by determining the transform on the next subgroup using the previous one. There is a similar construction $\{e\}<P_2 <\dots < P_n$. However, there is still no efficient $HSP$ algorithm known.

\textbf{Known partial results}:

For normal subgroups ({\it i.e.} $gK = Kg$ for all $g\in G$), we have
\begin{theorem*}[Hallgren, Russell, Tashina SIAMJ. Comp\_ vol32 p916-934 (2003)]\ 

Suppose $G$ has $\qft$ efficiently implementable. Then if the hidden subgroup $K$ is a normal subgroup, then there is an efficient quantum HSP algorithm.
\end{theorem*}

In particular, we find $\{\ket{g_0 K}\}$ and do $M_{\textrm{rep}}$ on it, and repeat $O(\log |G|)$ times. Then $K$ normal implies the outputs suffice to efficiently determine $K$.

\begin{theorem*}[Effinger, Heyer, Krull, 2004]
For general non-abelian HSP, $M = O(\poly \log |G|)$ random coset states $\ket{g_1 K},\dots,\ket{g_M K}$ suffice to determine $K$.
\end{theorem*}
 In particular, there is always efficient {\it query} complexity. Unfortunately, there is no known method to efficiently determine $K$ from the $M$ coset states (see Sheet 1 \# 7 for a proof of this theorem).
 
 The way we do this is to use a measurement procedure on $\ket{g_1K}\otimes \dots\otimes \ket{g_M K}$ that takes exponential time in $\log |G|$ to complete.
\end{remark*}
\marginpar{Lecture 6}

\section{Quantum Phase Estimation (PE) Algorithm}

This is a unifying principle for quantum algorithms, and again uses $\qft$. It has many applications, {\it e.g.} an alternative factoring algorithm to Shor's algorithm, due to A. Kitaev (Sheet 2 \#2).

\underline{Scenario}: we are given a unitary operator in $d$ dimensions, and an eigenstate $\ket{v_\phi}:U\ket{v_{\phi}} = \e^{2\pi i \phi}\ket{v_\phi}$. We want to estimate $\phi$, with $0 < \phi < 1$, to $n$ binary bits of precision. In particular we have $\phi = 0.i_1i_2\dots i_n\dots$ in binary, and we want to determine $i_1,\dots,i_n$ for any given $n$.

We will need the \undf{controlled-$U^k$} for integers $k$:
\begin{align*}
CU^k\ket{0}\ket{\xi} &= \ket{0}\ket{\xi}\\
CU^k\ket{1}\ket{\xi} &= \ket{1}(u\ket{\xi})\\
\end{align*}

Note also that $U^k\ket{v_\phi} = \e^{2\pi i k\phi}\ket{v_\phi}$, and $C(U^k) = (CU)^k$.

\begin{remark*}
Given $U$ as a formula or circuit description, we can readily implement $CU$ {\it e.g.} by controlling each gate of the circuit.

But if $U$ is given as a black box ({\it i.e.} physical operation) then we need further information in order to implement $CU$.

We can see this since $U$ as a black box is equivalent to $\e^{i\theta}U$ as a black box, but the controlled versions of these gates are different, since the $C$-$(\e^{i\theta}U)$ gate will switch global phase on $\ket{0} + \ket{1}$ to a local phase.

It in fact suffices to have an eigenstate $\ket{\alpha}$ with known/specified eigenvalue $\e^{i\alpha}: U\ket{\alpha} = \e^{i\alpha}\ket{\alpha}$. Then $\e^{i\theta}U$ has $\alpha \mapsto \alpha + \theta$.
\end{remark*}


We will actually want a ``generalised controlled-$U$'', given for $x \in \Z_n$:
\begin{align*}
	{\ket{x}}\ket{\chi} \mapsto U^x\ket{\chi}
\end{align*}
We can make it from $cU^k$ as follows. For $x = x_{n-1}\dots x_1x_0$ in binary... [diagram].

If input $\ket{\chi} = \ket{v_\phi}$, then we get $\e^{2\pi i\phi x}\ket{x}\ket{v_\phi}$.


\begin{theorem*}[PE]
If the measurements give output $\theta = 0.y_0y_1\dots y_{n-1}$, with $\phi = 0.z_0,\dots,z_{n-1}z_{n}\dots$ then
\begin{enumerate}[label=(\alph*)]
	\item $\P(\theta$ is closest $n$ binary digit approx to $\phi) \ge 4/\pi^2 \approx 0.4$
	\item $\P(|\theta - \phi| \ge \eps)$ is at most $O(1/2^n\eps)$, and we will show it is in fact $\le 1/2^{n+1}\eps$.
\end{enumerate}
\end{theorem*}

\begin{remark*}
In Theorem PE (a), we have probability $4/\pi^2$ that {\it all} $n$ lines of $n$-line PE process give correct bits.

But: if we want $\phi$ accurate to $m$ bits with probability $1 - \eta$, then use Theorem PE (b) with $\eps = 1/2^m$. Then we'll need $n > m$ lines with $\eta = 1/2^{n+1}\eps$ and $\eps = 1/2^m$, {\it i.e.} $n = m + \log (1/\eta) -1$, where $n$ is the number of lines in the PE algorithm, $m$ is the number of bits we want with high probability, and $\log(1/\eta) -1 $ is the number of additional bits we require to ensure this accuracy.
\end{remark*}

\marginpar{Lecture 7}

\begin{proof}
	Recall that we have
	\begin{align*}
		\qft^{-1}\ket{x} = \frac{1}{\sqrt{2^n}} \sum_{y=0}^{2^n-1}\e^{-2\pi i yx/2^n}\ket{y}
	\end{align*}
	
	So for the state $\ket{A}$ used earlier, we have
	\begin{align*}
		\qft^{-1}\ket{A} = \frac{1}{2^n} \sum_{y}\left[ \e^{2\pi i (\phi - y/2^n)x}\right]\ket{y}
	\end{align*}
	So for the measurement, the probability that we see the $n$-bit integer $y = y_0y_1\dots y_{n-1}$ is
	\begin{align*}
		\frac{1}{2^{2n}}\left|\sum_{x=0}^{2^n-1}\e^{2\pi i (\phi - y/2^n)x}\right|^2
	\end{align*}
	Letting $\delta(y)\coloneqq \phi - y/2^n)$, we see that we have a geometric series with ratio $\e^{2\pi i\delta(y)}$, solve
	\begin{align*}
		\P[\textrm{see }y] = \frac{1}{2^{2n}}\left| \frac{1-\e^{2^n\cdot 2\pi i \delta(y)}}{1 - \e^{2\pi i \delta(y)}} \right|^2
	\end{align*}
	We now aim to bound/estimate this expression, which we do by bounding the numerator/denominator appropriately. Note that:
	\begin{enumerate}[label=(\roman*)]
		\item $|1 - \e^{i\alpha}| = |2\sin(\alpha/2)| \ge \frac{2}{\pi}|\alpha|$ for $|\alpha| < \pi$
		\item $|1 - \e^{i\beta}| \le \beta$, by comparing the length of the chord and the arc between $1$ and $\e^{i\beta}$ on the unit circle
	\end{enumerate}

	So for (a), in the probability equation we use (i) with $\alpha = 2^n\cdot 2\pi \delta(a) < 2^n\cdot 2\pi/2^{n+1} < \pi$, and so $|1 - \e^{i\alpha}| \ge \frac{2}{\pi}|\alpha| = 2^{n+2}\delta(a)$, where $a$ is the closest $n$-bit string, so its distance from $\phi$ is small.

	Then we use (ii) with $\beta = 2\pi\delta(a)$ to upper bound the bottom line, and hence
	\begin{align*}
		\P[\textrm{see }a] \ge \frac{1}{2^{2n}}\left(\frac{2^{n+2}\delta(a)}{2\pi \delta(a)}\right) = \frac{4}{\pi^2}
	\end{align*}

	For (b), we upper bound the probability equation and sum over all $y$ such that $|\delta(y)| > \eps$.

	For the top line, we use that $|1 - \e^{i\alpha}| \le 2$ for any $\alpha$ (trivially), and for the bottom line we use (i), so $|1 - \e^{2\pi i\delta(y)}| \ge 4\delta(y)$. Hence
	\begin{align*}
		\P[y] \le \frac{1}{2^{2n}}\left(\frac{2}{4\delta(y)}\right)^2 = \frac{1}{2^{2n+2}\delta(y)^2}
	\end{align*}
	Now we sum over all $y$ such that $|\delta(y)| > \eps$; the $\delta(y)$ values are separated by $1/2^n$s, so let $\delta_+$ (resp. $\delta_-$) be the first $\delta(y)$ with $\delta(y)\ge \eps$ (resp. $\delta(y)\le -\eps$). So $|\delta_+|,|\delta_-|\ge \eps$.

	Then if $|\delta(y)| \ge \eps$ we have $|delta(y) = \delta_+ + \frac{k}{2^n}$, $k = 0,1,\dots$ or $=\delta_- - \frac{k}{2^n}$, $k = 0,1,\dots$. So $|\delta(y)|\ge \eps +\frac{k}{2^n}$, $k = 0,1,dots$ in each case.

	So
	\begin{align*}
		\P[|\delta(y)| \ge \eps] &\le 2\sum_{k=0}^{\infty}\frac{1}{2^{2n+2}}\frac{1}{(\eps+\frac{k}{2^{n}})^2}\\
		&\le \frac{1}{2}\int_{0}^{\infty}\frac{1}{2^n\eps + k)^2}\textrm{d}k\\
		&=\frac{1}{2}\int_{2^n\eps}^{\infty}\frac{\textrm{d}k}{k^2} = \frac{1}{2^{n+1}\eps}
	\end{align*}
\end{proof}

This is an extremely useful theorem, and we will see it in various places.

\begin{remark*}[Further Remarks on use of PE]\ 
	\begin{enumerate}
		\item If $C-U^{2^k}$ is implemented as $(C-U)^{2^k}$ then the PE algorithm needs exponential time in $n$, via $1 + 2 +\dots + 2^{n-1} = 2^{n}-1$ $C-U$ gates.
		
		But for some special $U$s, $U^{2^k}$ and $C-U^{2^k}$ can be implemented in $\poly(k)$ time, so we get the polynomial time PE algorithm. For instance, we can use repeated squaring to calculate exponents efficiently.

		This leads to an alternative factoring algorithm (more generally, order-finding or periodicity algorithm) due to Kitaev using PE.

		\item If instead of $\ket{v_\phi}$ we use general $d$-dimensional input state $\ket{\xi}$, what happens? Since we know what happens on eigenstates, and these form a basis, we have that
		\begin{align*}
			\ket{xi} &= \sum_{j}c_j\ket{v_{\phi_j}}\\
			U\ket{v_{\phi_j}} &= \e^{2\pi \phi_j}\ket{v_{\phi_j}}
		\end{align*}
		We then get (before the final measurement) a unitary process:
		\begin{align*}
			\ket{0\dots 0}\ket{\xi} \xrightarrow{U_{\textrm{PE}}} \sum_{j}c_j\ket{\phi_j}\ket{v_{\phi_j}}
		\end{align*}
		and the Born rule implies the final measurement gives a choice of one of the $\phi_j$s, chosen with probability $|c_j|^2$. In particular, you don't get an arbitrary average of the $\phi_j$s, you can precisely one of them (at random), which is still useful information.

		This works perfectly if $\phi_j$s have at most $n$ digits, the number of lines in the PE algorithm. Otherwise with $m$ lines and general $\phi_j$s we get a precision issue (usually swept under the rug); more precisely:

		Using Theorem PE(b), if we want $m$ bits correct with probability $1 - \eta$, we use $n = m + o(\log 1/\eta)$ lines, and $U_{\textrm{PE}}$ acts as above with $\ket{\phi_j}$ registers replaced by
		\begin{align*}
			(\sqrt{1-\eta})\ket{\tilde{\phi}_j}+\sqrt{\eta}\ket{\textrm{perp}}
		\end{align*}
		with $\ket{\textrm{perp}}\perp \ket{\tilde{\phi}_j}$, where $\ket{\tilde{\phi}_j}$ are all $n$ bit strings with the first $m$ bits correct for $\phi$, and $\ket{perp}$ all $n$ bit strings with the first $m$ not correct. Measurement will then give $\tilde{\phi}_j$ value with $m$ correct bits with probability $|c_j|^2(1-\eta)$.

		We often ignore this prceision issue, since we can make $\eta$ very, very small with modest $o(\log 1/\eta)$ overhead. The final state is $\eta$-close to state giving $\phi$ perfectly.
	\end{enumerate}

\end{remark*}

\section{Amplitude Amplification}

	This is the apotheosis of technique in Grover's algorithm.

	\underline{Some background}:

	\begin{remark*}[Reflection operation] Recall:
		\begin{itemize}
			\item state $\ket{\alpha}$ in $\mathcal{H}_d$ gives a $1$-dim subspace $L_\alpha$, and $(d-1)$-dime orthogonal complement $L_\alpha^{\perp}$
			\begin{align*}
				I_{\ket{\alpha}} &\coloneqq I - 2\ket{\alpha}\bra{\alpha}\\
				\therefore I_{\ket{\alpha}}\ket{\alpha} &= - \ket{\alpha}\\
				I_{\ket{\alpha}}\ket{\beta} &= \ket{\beta}\ \textrm{for any }\ket{\beta}\perp\ket{\alpha}
			\end{align*}
			\textit{i.e.} $I_{\ket{\alpha}}$ is reflection in $(d-1)$-dim mirror $L_\alpha^\perp$.

			Note for any unitary $U$, $UI_{\ket{\alpha}}U^\dagger = I_{U\ket{\alpha}}$

			\item For $k$-dim subspace $A\subseteq \mathcal{H}_d$ with any orthonormal basis $\ket{a_1},\dots \ket{a_k}$, $P_A = \sum_{i=1}^{k} \ket{a_i}\bra{a_i}$ is the projection operator into $A$ (note that this is independent of the choice of orthonormal basis). Then we have
			\begin{align*}
				I_A &\coloneqq I - 2P_A\\
				\implies I_A\ket{\xi} &= \ket{\xi}\textrm{ if }\ket{\xi}\in A^{\perp}\\
				I_A\ket{\xi} &= -\ket{\xi}\textrm{ if }\ket{\xi}\in A
			\end{align*}
			So $I_A$ is relection in the $(d-k)$-dim mirror $A^\perp$.
		\end{itemize}
	\end{remark*}

\subsection*{Recap of Grover's Algorithm}

\begin{enumerate}[label=-]
	\item search for unique `good' item in an unstructured database of $N = 2^n$ items. This is formalised as:
	
	Given an oracle for $f$:$n$-bits $\ra$ $1$-bit.

	Promise: there is a unique $x_0 \in n$-bits with $f(x_0) = 1$.

	Problem: find $x_0$.

	\item closely related to NP \& Boolean satisfiability problem
	
\end{enumerate}
\begin{itemize}
	\item Using one query to the $(n+1)$-qubit $U_f$, we can implement the reflection operator:
	\begin{align*}
		I_{\ket{x_0}}:\ket{x_0} \ra \left\lbrace \begin{array}{cc} \ket{x} & x\ne x_0 \\ -\ket{x} & x = x_0 \end{array}\right.
	\end{align*}

	Visually, we apply $U_f$ to $\ket{x}\left(\frac{\ket{0} - \ket{1}}{2}\right)$ and discard the last qubit.

	\item Then consider the Grover iteration operator on $n$ qubits
	\begin{align*}
		Q\coloneqq -H_nI_{\ket{0}}H_nI_{\ket{x_0}} = -I_{\ket{\psi_0}}I_{\ket{x_0}}
	\end{align*}
	where $H_n = H\otimes \dots\otimes H$, $\ket{\psi_0} = H_n\ket{0\dots0} = \frac{1}{\sqrt{2^n}}\sum_{x}\ket{x}$. So one application of $Q$ uses 1 query to $U_f$.
\end{itemize}

\begin{theorem*}[Grover 1996]
	In the $2$-dimensional span of $\ket{\psi_0}$ and (unknown) $\ket{x_0}$ the action of $Q$ is rotation by angle $2\alpha$ where $\sin\alpha = \frac{1}{\sqrt{N}}$.
\end{theorem*}
Hence to find $x_0$ given $U_f$:
\begin{enumerate}
	\item Make $\ket{\psi_0}$
	\item Apply $Q$ $m$ times, where
	\begin{align*}
		m = \frac{\arccos \frac{1}{\sqrt{N}}}{2\arcsin\frac{1}{\sqrt{N}}} = \frac{\beta}{2\alpha}
	\end{align*}
	with $\cos\beta = \frac{1}{\sqrt{N}}$, $\beta$ being the angle between $\ket{\psi_0}$ and $\ket{x_0}$ (nearly orthogonal).

	This rotates $\ket{\psi_0}$ very close to $\ket{x_0}$ (within $\pm \alpha$)

	\item Measure, to see $x_0$ with high probability $\approx \cos^2\alpha  = 1 - \sin^2\alpha = 1 - \frac{1}{N}$.
	
	For large $N$, $\arccos\frac{1}{\sqrt{N}} \approx \frac{\pi}{2}$ and $\arcsin\frac{1}{\sqrt{N}} \approx \frac{1}{\sqrt{N}}$. So $m = \frac{\pi}{4}\sqrt{N}$ iterations/queries to $U_f$ are sufficient (it can be shown this is also necessary).
\end{enumerate}

Classically, we need $O(N)$ queries to see $x_0$ with any constant probability (independent of $N$), so we have obtained a \underline{square root} time speed up quantumly.

\subsection*{Amplitude Amplification Problem}

Let $G$ be any subspace (``good subspace'') of state space $\mathcal{H}$ and let $G^{\perp}$ be its orthogonal complement, such that $\mathcal{H} = G \oplus G^\perp$ (the latter is the ``bad subspace'').

Given any $\ket{\psi}\in \mathcal{H}$, we have unique decomposition with real non-negative coefficients:
\begin{align*}
	\ket{\psi} = \sin\theta \ket{g} + \cos\theta\ket{b}
\end{align*}
with $\ket{g} \in G$ and $\ket{b}\in G^\perp$.

We now introduce reflections that flip $\ket{\psi}$ and good vectors:
\begin{align*}
	I_{\ket{\psi}} &= I - 2\ket{\psi}\bra{\psi}\\
	I_G &= I - 2P_G
\end{align*}
So $\sin \theta = ||P_G\ket{\psi}|| = $ length of good projection of $\ket{\psi}$.

Then introduce $Q\coloneqq -I_{\ket{\psi}}I_G$. This is a generalisation of Grover's Algorithm to any subspace.

\begin{theorem*}[Amplitude Amplification]
	In the $2$-dimensional subspace spanned by $\ket{\psi}$ and $\ket{g}$ (or equivalently by orthonormal vectors $\ket{g}$ and $\ket{b}$), $Q$ is rotation by $2\theta$ where $\sin \theta = $ length of good projection of $\ket{\psi}$.
\end{theorem*}

\begin{proof}
	W have $I_G\ket{g} = - \ket{g}$, $I_G\ket{b} = \ket{b}$ so
	\begin{align*}
		Q\ket{g} &= -I_{\ket{\psi}}I_{G}\ket{g} = I_{\ket{\psi}}\ket{g}\\
		Q\ket{b} &= -I_{\ket{\psi}}\ket{b}
	\end{align*}
	Now
	\begin{align*}
		I_{\ket{\psi}} &= I - 2(\sin\theta\ket{g} + \cos\theta\ket{b})(\sin\theta\bra{g} + \cos\theta\bra{b})\\
		&= I - 2\big[\sin^2\theta \ket{g}\bra{g} + \sin\theta\cos\theta\ket{g}\bra{b}+\sin\theta\cos\theta\ket{b}\bra{g}+\cos^2\theta\ket{b}\bra{b}\big]
	\end{align*}
	And direct calculation (using $\langle g | b \rangle = 0$, $\langle g | g\rangle = \langle b | b \rangle = 1$) gives:
	\begin{align*}
		Q\ket{b} &= \cos2\theta\ket{b} + \sin2\theta \ket{g}\\
		Q\ket{g} &= -\sin2\theta \ket{b} + \cos2\theta \ket{g}\\
		&= I\ket{g} - 2\sin^2\theta\ket{g} - 2\sin\theta\cos\theta \ket{b}\\
		&= (1-2\sin^2\theta)\ket{g} - 2\sin\theta\cos\theta\ket{b}\\
		&= \cos2\theta \ket{g} - sin2\theta \ket{b}
	\end{align*}
	So in the $\{\ket{b},\ket{g}\}$ basis, the matrix of $Q$ is
	\begin{align*}
		Q = \left[\begin{array}{cc} \cos2\theta & -\sin2\theta \\ \sin2\theta & \cos 2\theta \end{array}\right]
	\end{align*}
	Which is a rotation through $2\theta$.
\end{proof}

We can then very simply observe that $Q^n\ket{\psi} = \sin((2n+1)\theta)\ket{g} + \cos((2n+1)\theta)\ket{b}$, and if we measure $Q^n\ket{\psi}$ for good vs bad, then $\P[\textrm{good}] = \sin^2(2n+1)\theta$. This is maximised when $(2n+1)\theta = \pi/2$, \textit{i.e.} $n = \pi/4\theta - \frac{1}{2}$ (taken to the nearest integer).

\begin{remark*}[Example]
If we had $\theta = \pi/6$, then $n = \pi/4\theta - \frac{1}{2} = 1$ (exactly), and $Q^1$ rotates $\ket{\psi}$ exactly onto $\ket{g}$, hence we obtain a good result with certainty.
\end{remark*}

Generally, for given $\theta$, $n$ is not an integer so we instead take the nearest integer to $\pi/4\theta - \frac{1}{2} \approx \pi/4theta$ for small $\theta$, and $Q^n\ket{\psi}$ will be within angle $\pm \theta$ of $\ket{g}$. So $\P[\textrm{good}]\ge \cos^2\theta \approx 1 - o(\theta^2)$.

All of this can be implemented if $I_{\ket{\psi}}$ and $I_{G}$ can be implemented (see ES2).

For $I_G$, it suffices for $G$ to be spanned by computational basis states $\ket{x}$s and the indicator function ($f(x) = 1$ for $x$ good and $f(x) = 0$ for $x$ bad) is efficiently computatble.

For $I_{\ket{\psi}}$: often have $\mathcal{H} = n$-qubits and $\ket{\psi} = H_n\ket{0\dots 0}$, and then $I_{\ket{W\psi}}$ can be implemented in $O(n)$ time.

\begin{remark*}[Notes]\ 
	\begin{enumerate}
		\item In the AA process, relative amplitudes of good labels in $\ket{g}$ staty the same as they were in $\ket{\psi} = \sin\theta\ket{g} + \cos\theta \ket{b}$, and AA amplifies the overall $\ket{g}$ amplitude at the expensive of the $\ket{b}$ amplitude.
	
		This is useful for state preparations. For instance:
		\begin{align*}
			\sum_{x\in (\Z/N\Z)^\times}\ket{x}
		\end{align*}

		\item Final state is generally not exactly $\ket{g}$, but if $\sin\theta$ is known, then we can modify the AA process to make it exact, with only a very modest increasee in queries to the oracle - so we can obtain $\ket{g}$ with certainty (ES2).
	\end{enumerate}
\end{remark*}

\subsection*{Applications of AA}
\ 
\begin{enumerate}
	\item Grover search with one or more $(k)$ good items in $N = 2^n$
	\begin{align*}
		\ket{\psi} &= \ket{\psi_0} = \frac{1}{\sqrt{2^n}}\sum_{x\in \Z_2^n}\ket{x}\\
		&= \sqrt{\frac{k}{N}}\left(\frac{1}{\sqrt{k}}\sum_{\textrm{good}}\ket{x}\right) + \sqrt{\frac{N-k}{k}}\left(\frac{1}{\sqrt{N-k}}\sum_{\textrm{bad}}\ket{x}\right)
	\end{align*}

	$G$ spanned by the good $\ket{x}$s. $\sin\theta = \sqrt{\frac{k}{N}}$ so $Q$ is rotation through $2\theta$, where $\theta\coloneqq \arcsin\sqrt{\frac{k}{N}} \approx \sqrt{\frac{k}{N}}$ (since $N$ is often much larger than $k$ - if not then this whole problem is roughly a non-issue, just pick randomly).

	So $O(\sqrt{N/k})$ queries needed.

	Note that if $N = 4$, $n = 2$, $k = 1$ then $\theta = \pi/6$, $2\theta = \pi/3$, so one query gives the good item of one in four with certainty - despite being very simple, this is rather remarkable as classicly this is clearly impossible.


	\item \underline{Square-root speedup of general quantum algorithms}
	
	Let $A$ be a quantum algorithm/circuit (\textit{i.e.} is the unitary operator for the whole circuit), let the input state be $\ket{0\dots0}$; so the final state is $A\ket{0\dots0}$. The good labels are the desired computational outcomes.

	Write $A\ket{0\dots 0} = \alpha\ket{a} + \beta\ket{b}$, $\alpha = \sin\theta$ where $\ket{a}$ is a normalised, generally unequal supervision of good labels $\sum_{\textrm{good }x}c_x\ket{x}$.

	So $\P[\textrm{success in 1 run}] = |\alpha|^2$, so we need $o(1/|\alpha|^2)$ repetitions of $A$ to succeed with any given constant (high) probability $1 - \eps$.

	However, we can instead use AA!

	Note that we assume that we can easily check if the answer is good or bad (this is characteristic of NP, where it is hard to find an answer but easy to verify one). This assumption is so that we can then implement $I_G:\ket{x}\ra -\ket{x}$ if $x$ good, and $\ket{x}$ if $x$ bad (reflection in the good labels).

	Consider $\ket{\psi} = A\ket{0\dots0}$ and
	\begin{align*}
		Q = -I_{A\ket{0\dots0}}I_G = - AI_{\ket{0\dots0}}A^\dagger I_G
	\end{align*}
	so all parts are implementable: it is easy to invert $A$ as we have the circuit, $I_G$ is as above, and $I_{\ket{\psi_0}} = I - 2\ket{0\dots0}\bra{0\dots0}$ (\textit{c.f.} ES2).

	So by the Amplitude Amplification theorem $Q$ is rotation through $2\theta$ with $\sin\theta = |\alpha|$. So after $n\approx \pi/4\theta = o(1/\theta) = o(1/\sin\theta) = o(1/|\alpha|)$ (small $\alpha$) repetitions, $A\ket{0\dots0}$ will be rotated very near to $\ket{g}$ and the final measurement will succeed with high probability.

	So we get a square root speedup over the direct method.

	Moreover, if the success probability of $A$ (\textit{i.e.} $|\alpha|^2$) is \underline{known} then the exact ``improved modified'' AA process converts the probabilistic algorithm $A$ into a deterministic one (get the outcome with certainty - this isn't too important in practice, but is theoretically interesting).

	\item \underline{Quantum Counting 1998} (uses AA \& PE together)

	Given $f:\Z_2^n\ra\Z_2$ with $k$ good $x$s, we want to estimate $k$ (rather than just find some good $x$). These counting problems in complexity theory are known to be significantly more difficult than more simple existence problems.

	Recall that the Grover operator $Q_G$ for $f$ is rotation through $2\theta$ in $2$-dimensional plane of $\ket{\psi_0} = \frac{1}{\sqrt{2^n}}\sum_{x\in\Z_2^n}\ket{x}$ and its good projection $\ket{g} = \frac{1}{\sqrt{k}}\sum_{x\textrm{ good}}\ket{x}$ with $\sin\theta = \sqrt{k/N}\approx \theta$ for $k <<N$.

	Also recall facts about 2D rotation through $2\theta$ in the $\{\ket{b},\ket{g}\}$ (o.n.b) plane. We have eigenvectors $\ket{e_{\pm}} = \frac{1}{\sqrt{2}}(\ket{b}\pm i\ket{g})$, with eigenvalues $\lambda_{\pm} = \e^{\pm2i\theta}$ (this is true for any o.n.b replacing $\ket{b},\ket{g}$).

	Then check $\ket{\psi_0} \coloneqq \sin\theta\ket{g} + \cos\theta\ket{b} = \frac{1}{\sqrt{2}}(\e^{i\theta}\ket{e_+}+\e^{-i\theta}\ket{e_-})$ - that is to say this is an equally weighted superposition of $\ket{e_{\pm}}$s.

	For PE application we write eigenvalues $\e^{\pm2i\theta}$ as $\e^{2\pi i\phi_{\pm}}$ with $0\le \phi_{\pm} < 1$. We have $\phi_+ = (2\theta)/(2\pi) = \theta/\pi$, and $2\pi \phi_- = -2\theta+2\pi$ so $\phi_- = 1 - \theta/\pi$ (note $\theta/pi = \frac{1}{\pi}\sqrt{\frac{k}{N}}$).

	Hence: running the PE algorithm with $U = \textrm{Grover }Q$ and eigenstate register set to $\ket{\phi_0}$ will output (an approximation to) $\theta/\pi$ or $1 - \theta/\pi$ with probability $1/2$. For small $\theta$ these are easily distinguishable. Hence in either case we get an approximation to $\theta = \sqrt{k/N}$.

	More precisely (with $n$ as above), for any $m$, running PE with $m$ qubit lines gives an $m$-bit approximation to $\sqrt{k/N}$ and uses $2^m$ $C-Q$ gates, \textit{i.e.} $2^m$ queries to $f$. So by theorem PE(a) we learn $\sqrt{k/n}$ to additive error $O(1/2^m)$ with constant probability $4/\pi^2$ using $O(2^m)$ queries to $f$.

	Write $\frac{1}{2^m}$ as $\frac{\delta}{\sqrt{N}}$. So $\delta$ is the error in $\sqrt{k}$ directly. So we learn $\sqrt{k}$ to additive error $O(\delta)$ using $O(2^m) = O(\sqrt{N}/\delta)$ queries. Therefore we learn $k$ itself to additive error $O(\delta\sqrt{k})$ using $O(\sqrt{N}/\delta)$ queries.

	But classically, the same approximation (obtained with constant probailitiy) this requires $O(N/\delta^2)$ queries, which is \underline{quadratically} more.

\end{enumerate}

\section{Hamiltonian Simulation}

The aim here is to use a quantum computer to simulate the evolution/dynamics of a quantum system, given its Hamiltonian $H$. This is an extremely important and very active area of quantum research, and is expected to be one of the first useful real-world applications of quantum computing. The reason is that studying the behavior of molecules of even relatively modest length becomes extremely difficult classically, but is extremely useful for developing new drugs with the correct intended effects.

For $n$ qubits, we generally need $O(2^n)$ time on a classical computer, but we will do it in $\poly(n)$ time (for a suitable class of $H$s).

\subsection*{Hamiltonians \& Quantum Evolution}

We have a physical system: a state $\ket{\psi}$, with a Hamiltonian $H$. A Hamiltonian is a self-adjiont (Hermitian) operator/matrix, the physical interpretation of which is the ``energy observable''. Time evolution is given by the Schrodinger equation ($\bar{h} = 1$):
\begin{align*}
	\frac{\textrm{d}}{\textrm{d}t}\ket{\psi(t)} = -iH\ket{\psi(t)}
\end{align*}
so the solution is characterised by $H$. We remark that the Hamiltonian can in general be time-dependent, but we will only consider the time independent case.

Formally then, $\ket{\psi(t)} = \e^{-iHt}\ket{\psi(0)}$ where the exponential given is the matrix exponential defined by the series $\e^A = I + A + A^2/2! +\dots$ for any square matrix $A$. Moreover, if $H$ is Hermitian than $\e^{iH}$ is always unitary. Thus our problem is as follows:

Given $H$ and a time $t$, we want to simulate the action of a unitary $U(t) = \e^{-iHt}$ to a suitably good approximation.

We already have a notion for what it is for two vectors to be `close', but we need to formalise a notion of closeness between (unitary) operators; for this we use the operator norm (spectral norm) given by
\begin{align*}
	||A|| &= \max_{||\ket{\psi}||=1}||A\ket{\psi}||\\
	&= |\textrm{max eigenvalue}|\quad\textrm{if $A$ diagonalisable}
\end{align*}
It is easy to see that:
\begin{itemize}
	\item $||A+B|| \le ||A|| + ||B||$
	\item $||AB|| \le ||A||\cdot||B||$
\end{itemize}

We then say that $U$ approxiamtes $\tilde{U}$ to within $\eps$ if $||U - \tilde{U}|| < \eps$; this has the geometrical interpretation that for any normalised $\ket{\psi}$ the vectors $U\ket{\psi}$ and $\tilde{U}\ket{\psi}$ are at most $\eps$ apart.

\subsection*{$k$-local Hamiltonians}

$H$ on $n$ qubits is a $2^n\times 2^n$ Hermitian matrix. We'll want to simulate $U = \e^{-iHt}$ with a circuit of size $\poly(n,t)$ using \underline{basic} unitary gates (so the operations themselves don't grow too large, negating the speed up by circuit length) and size $\poly(1/\eps)$ for $\eps$-approxiamtion.

Note that not all $H$s can be efficiently simulated, but some important physical classes can be.

\begin{defin*}[$k$-local]
	$H$ is $k$-local ($k$ fixed constant) on $n$ qubits if $$H = \sum_{j=1}^{m} H_j$$ where each $H_j$ is a Hermitian matrix acting on at most $k$ qubits (not necessarilyh continguous), \textit{i.e.} each $$H_j = \tilde{H}_j \otimes I$$, where $\tilde{H}_j$ acts on some $k$ qubits and $I$ is the identity on the rest.
\end{defin*}

So this is still an exponentially large matrix, but it is a description of constant size. So we must have $m\le \binom{n}{k} = O(n^k) = \poly(n)$ terms in $H$, each with constant size description. So the whole Hamiltonian can be specified with $\poly(n)$ size; generally though this is not the case since there are exponentially many degrees of freedom.

\begin{remark*}[Examples]\ 
	\begin{enumerate}[label=\arabic*)]
		\item $H = X\otimes I\otimes I - 5Z\otimes I\otimes Y$ is $2$-local on $3$ qubits.
		\item Write $M_{(k)}$ to denote the $1$-qubit operator $M$ acting on the $k^{\textrm{th}}$ (and $I$ on all the others). The \undf{Ising model} on an $(n\times n)$ square lattice of qubits is
		\begin{align*}
			H = J\sum_{i,j=1}^{n-1}Z_{(i,j)}Z_{(i,j+1)} + Z_{(i,j)}Z_{(i+1,j)}
		\end{align*}
		these pairs act on all nearest neighbours on the square lattice.

		The \undf{Heisenberg Model} on a line is
		\begin{align*}
			H = \sum_{i=1}^{n-1}J_xX_iX_{i+1} + J_yY_iY_{i+1}+J_zZ_iZ_{i+1}
		\end{align*}
		where $J,J_x,J_y,J_z$ are all constants.
	\end{enumerate}
\end{remark*}

Note: we will want to exponentiate a Hamiltonians, but in general $\e^{-i\sum_j H_jt}\ne \prod_j \e^{-iH_jt}$ if the $H_j$s don't commute.

But: $\e^{-iH_jt}$s are \underline{local} unitary gates, \it{i.e.} acting on $k$ qubits each, and we'll simulate $U(t_0) = \e^{-i\sum_jH_jt_0}$ in terms of $\e^{-iH_jt}$s (even if they don't commute) for suitably small values of $t$ - and we'll have a $\poly(n,t_0)$-sized circuit too, as required.

If we want to use some standard universal gate set (to further express the gates above) then we invoke:

\begin{theorem*}[Solovay-Kitaev Theorem]
	Let $U$ be a unitary operator on $k$ (constant) qubits and $S$ any universal set of quantum gates. Then $U$ can b eapproximated to within $\eps$ using $O(\log^c(1/\eps)$ gates from $S$ with $c < 4$ (but exponential in $k$, if $k$ is varying).
\end{theorem*}
\begin{proof}
	Omitted. See \it{e.g.} Nielsen \& Chuang.
\end{proof}

In particular, this means that all $S$-products of length $O(\log^c(1/\eps)$) will get within $\eps$ of any element of $U(k)$.

We'll also need a lemma about accumulation of error (\it{c.f.} ES3):

\begin{thmenv*}[Lemma A]
	Let $\{U_i\}$ be sets of $m$ unitary operators with $||U_i-V_i||\le \eps$  for all $i = 1,\dots,n$. Then $||U_m\dots U_1-V_m\dots V_1|| \le m\eps$, \it{i.e.} errors accumulate only \underline{linearly}
\end{thmenv*}
\begin{proof}
	Easy induction.
\end{proof}

\subsection{Hamiltonian Simulation for local Hamiltonians}

\underline{Warm-up}: the (easy) commuting case.

\marginpar{Lecture 11}

\begin{thmenv*}[Proposition]
Let $H = \sum_{j=1}^m H_j$ be any $k$-local Hamiltonian with $H_j$'s commuting. Then for any $t$, $\e^{-iHt}$ can be approximated to within $\eps$ by a circuit with $O(m\poly(\log(m/\eps))$ gates for any given universal set.
\end{thmenv*}
\begin{remark*}[Note]
	As $m = \binom{n}{k} = O(n^k)$, this is $\poly(n,\log 1/\eps)$ too, and $\log(1/\eps) = $ number of bits of precision in the approximation.
\end{remark*}
\begin{proof}
	The $H_j$s commute, so $\e^{-i\sum_j H_j t} = \prod_{j=1}^{m}(\e^{-iH_jt})$, and each $\e^{-iH_jt}$ is a $k$-qubit gate. Then by Solovay-Kitaev (SK), each $\e^{-iH_jt}$ can be approximated to within $\eps/m$ with $O(\poly(\log m/\eps))$ gates. Then by Lemma A, the full product $\prod_{j=1}^{m}$ is approximated to within $m(\eps/m) = \eps$, using a total of $O(m\poly(\log m/\eps))$ gates.
\end{proof}

Now for the full non-commuting case:

\begin{remark*}[Notation]
	For any matrix $X$, write $X + O(\eps)$ for any $X + E$ with $||E|| = O(\eps)$.
\end{remark*}

\begin{thmenv*}[Lemma B (Lie-Trotter Product Formula)]
	Let $A,B$ be matrices with $||A||,||B|| \le K$, with $K < 1$ suitably small. Then $\e^{-iA}\e^{-iB} = \e^{-i(A+B)} + O(K^2)$.
\end{thmenv*}
\begin{proof}
	Recall that:
	\begin{align*}
		\e^{-iA} &= I - iA + \sum_{k=2}^{\infty}\frac{(-iA)^k}{k!}\\
		&= I - iA + (iA)^2\sum_{k=0}^{\infty}\frac{(-iA)^k}{(k+2)!}
	\end{align*}
	and in fact we have, by the triangle inequality:
	\begin{align*}
		\left|\left|\sum_{k=0}^{\infty}\frac{(-iA)^k}{(k+2)!}\right|\right| & \le \sum_{k=0}^{\infty}\frac{||(-iA)||^k}{k!}\\
		&= e^{||A||} \le e^K < e
	\end{align*}
	Therefore $\e^{-iA} = I - iA + O(K^2)$. $(\ast)$ So:
	\begin{align*}
		\e^{-iA}\e^{-iB} &= (I - iA + O(K^2))(I - iB + O(K^2))\\
		&= I - i(A+B) + O(K^2)\\
		&= \e^{-i(A+B)} + O(K^2)
	\end{align*}
\end{proof}
	Now we apply this repeatedly to accumulate the sum of our matrices $H_1,\dots,H_n$ in the exponent.
\begin{itemize}
	\item \underline{Note}: If each $||H_i|| < K$ then $||H_1 + \dots + H_\ell|| < \ell K$, and we'll want this to be $<1$ for all $\ell \le m$. So for now, we'll assume $K < 1/m$ in order to have Lie-Trotter at all stages.
	\item Also take $t = 1$ for now (will do general $t$ later).
\end{itemize}

Given these, consider:
\begin{align*}
	\e^{-iH_1}\e^{-iH_2}\dots\e^{-iH_m} &= \left[ \e^{-i(H_1+H_2)} + O(K^2)\right] \e^{-iH_3}\dots \e^{-iH_m}
\end{align*}
Note now that $||H_1 + H_2|| < 2k$, and note also that the error generated at each state says the same with further unitary $U$s; $||AU|| = ||A||$. So it can be pulled through the remainder of the product:
\begin{align*}
	&= \e^{-i(H_1+H_2)}\e^{-iH_3}\dots\e^{-iH_m}+O(K^2)\\
	&= \left[\e^{-i(H_1+H_2+H_3)} + O((2K)^2)\right]\e^{-iH_4}\dots\e^{-iH_m}+O(K^2)\\
	&\ \ \vdots\\
	&= \e^{-i(H_1+H_2+\dots+H_m)}+O(K^2) + O((2K)^2) + \dots + O([(m-1)K]^2)\\
	&= \e^{-i(H_1 + H_2 +\dots + H_m)} + O(m^3K^2)
\end{align*}
We write this error as $Cm^3K^2$, and refer to it as ERR1.

Now for general finite $||H_j||$s and $t$ values:

$||H_jt||<Kt$ can be \it{large}, so introduce $N$ (which will be large-ish, chosen later) and note $H_jt/N$ has norm $\tilde{K} = ||H_jt/N|| < Kt/N$ $(\ast)$, which can be suitably \it{small}. \it{i.e.} we divide $t$ up into (small) $1/N$ intervals, and $U = \e^{i(H_1+\dots +H_m)t} = \left[\e^{i(H_1t/N+\dots +H_mt/N)}\right]^N$.

We want the final error for $U$ to be $<\eps$, so by Lemma A we want error for $[\dots]$ to be $< \eps /N$ \it{i.e.} from (ERR1) and $(\ast)$, we want $Cm^3\tilde{K}^2 < \eps /N$. So $Cm^3K^2t^2/N^2 < \eps /N$, and in particular $N\ge Cm^3K^2t^2/\eps$ (ERR2). If $N$ is this large, then $$||\e^{-iH_1t/N}\e^{-iH_2t/N}\dots\e^{-iH_mt/N} - \e^{-i(H_1+\dots+H_m)t/N}|| < \eps/N$$ Then by Lemma A again, $$||(\e^{-iH_1t/N}\dots\e^{-iH_mt/N})^N - \e^{-i(H_1+\dots+H_m)t}|| < \eps$$ The product on the left is $Nm$ gates of the form $\e^{-iH_jt/N}$, where $N$ is given by (ERR2), so the circuit size is $O(m^4(Kt)^2/\eps)$.

Recall for $n$ qubits and $k$-local Hamiltonians $m = O(n^k)$, so circuit size overall is $O(n^{4k}(Kt)^2/\eps) = O(n^{4k},t^2,1/\eps)$.


\begin{itemize}
	\item We have circuit $\mathcal{C}$ of size $|\mathcal{C}| = O(m^4 (Kt)^2/\eps)$ gates of the form $\e^{-iH_jt/N}$ approximating $U = \e^{iHt}$ to $O(\eps)$. If we want to use a standard universal set, then lemma A implies each of these $\e^{-iH_jt/N}$ gates needs to be approximated to $O(\eps/|\mathcal{C}|)$ to maintain overall $O(\eps)$ error for $U$. So by SK thm we need $O(\log^c(|\mathcal{C}|/\eps))$ gates from the universal set for each, \it{i.e.} get an extra (very modest!) multiplicative factor of $O(\log^c(m^4(Kt)^2)/\eps^2))$ in $|\mathcal{C}|$.
	
	\item For fixed $n,\eps$ and variable $t$, the quantum process $\e^{-iHt}$ runs for time $t$, but the quantum circuit simulation above runs for time $O(t^2)$. By refining the Lie-Trotter Lemma, it can be shown that this can be improved to $O(t^{1+\delta})$ for any $\delta > 0$.
\end{itemize}

\marginpar{Lecture 12}

\subsection*{Harrow Hassidim Lloyd (HHL) Quantum Algorithm for Systems of Linear Equations (2009)}

We'll want to solve a linear system $A\underline{x} = \underline{b}$, $\underline{b},\underline{x} \in \C^N$ where dimension $N$ is potentially very large.

\begin{itemize}
	\item Write $N = 2^n$ (or $2^n$ least power of $2$ greater than $N$)
	\item Rather than outputting the full solution $\underline{x}$ itself (would take at least $O(N)$ time), instead we compute (suitably approximations to) the value of properties of the solution $\underline{x}$, such as quadratic expressions $\underline{x}^\dagger M\underline{x}$, \it{e.g.} total weight of some subset of components
\end{itemize}

Very large systems of such equations are important in applications

\begin{itemize}
	\item data mining/machine learning on data sets of very large size (terabytes/petabytes)
	\item numerical solutions of PDEs - discretisation techniques (finite element methods) lead to linear systems of size far larger than the original problem description
\end{itemize}

The best known classical techniques take $\poly(N)$ time to solve such problems - this is true generally even to just compute properties of the solution as described above; in fact no better method is known than first computing the whole solution itself.

\textbf{Important parameters (both classical and quantum algorithms)}

\begin{itemize}
	\item system size $N$
	\item the desired approximation tolerance $\eps$
	\item the condition $\kappa$ of the matrix $A$j, defined as the ratio of the largest to smallest eigenvalue size: $$\kappa = \left|\frac{\lambda_{\max}}{\lambda_{\min}}\right|$$
	Note that $\kappa = \infty$ if $A$ is not invertible. The condition number provides a measure of how close $A$ is to being non-invertible. If we rescale $A$ to have $\lambda_{\max}= 1$, then $|\lambda_{\min}| = 1\kappa$ and numerical computation of $A^{-1}$ becomes less stable with increasing $\kappa$, needing more significant digits of computation and correspondingly longer runtime.
\end{itemize}

\subsubsection*{Preliminary requirement for the HHL algorithm}

\underline{Aim}: to compute $\eps$-approximation to properties of the solution of $N$-dimensional system $Ax = b$ in time $\poly(\log N) = \poly(n)$. We'll take the property to be a quadratic expression of the form $\mu = x^\dagger M x$.

\underline{Immediate issue:} how the defining ingredients $A,b$ are actually given \it{e.g.} if simply written out as components, then it takes $\poly(N)$ time just to write them before starting!

Thus we need a different presentation, but one that is still available in important applications.

The $\poly(\log N)$ runtime will be achieved usingo only $O(\log N)$ qubits, and it will never be required in the algorithm to write down all of $A,b$ or $x$.

\underline{List of starting requirements}:

For the matrix $A$:

\begin{enumerate}
	\item We will require that $A$ is Hermitian.
	
	\underline{Remark}: If it is not, just double the size of the system (no effect on time complexity), and solve
	\begin{align*}
		\left( \begin{array}{cc} 0 & A^\dagger \\ A & 0 \end{array}\right)\left(\begin{array}{c} x \\ y \end{array}\right) = \left( \begin{array}{c}0 \\ b \end{array}\right)
	\end{align*}
	which is $A^\dagger y = 0$ and $Ax = b$, so $y = 0$ (since $A$ invertible).

	\item We will require that the condition number $\kappa$ of $A$ is ``suitably small'', in particular that $\kappa$ is bounded by $\poly(n)$ with increasing $N\sim 2^n$. Such matrices/linear systems are called \undf{well-conditioned}. We will also assume for convenience that $A$ (and RHS $b$) has been rescaled to have $\lambda_{\max} = 1$, so all sizes of eigenvalues lie in $[1/\kappa,1] = [1/\poly(n),1]$.
	
	\item We will need to implement Hamiltonian simulation for $\e^{iAt_0}$ (with suitable $t_0$, see later). Correspondingly, we require that $\e^{iAt_0}$ can be implemented in time (circuit size) $\poly(n,t_0)$.
	
	\underline{Remark}: Any class of (Hermitian, well-conditioned) matrices $A$ having this property will do for HHL. This is the only place that the presentation of $A$ features in HHL.
\end{enumerate}

In the literature, a particular class (that's still broadly applicable) is often described. These are matrices that are \undf{row-sparse} and \undf{row-computable}, defined as follows:

\begin{enumerate}[label = (\roman*)]
	\item $A$ is \undf{row-sparse} if each row contains at most $\poly(n)$ non-zero entries (amongst its $N \sim 2^n$ entries). More generally, $A$ is \undf{row $s$-sparse} if each row now contains at most $s$ non-zero entries. [Note that no $s$ implies $s = \poly(n)$.]
	\item An $s$-sparse matrix $A$ is called \undf{row-computable} if the entries of $A$ can be efficiently computed in the following sense (instead of, say, being given inefficiently as a list of $N^2$ values):
	
	There is a (classical) $O(s)$-time computation which given any row index $i$, $1\le i\le N$ and integer $k$, $1\le k \le N$ outputs the $k^{\th}$ non-zero entry $A_{ij}$ of row $i$, and its column location $j$. \it{I.e.} $C(i,k) = (j,A_j)$. If $k$ exceeds the number of non-zero entries, we can set $C(i,k) = (i,0)$ as a flag that there are none left.
\end{enumerate}


\begin{theorem*}[Hamilton Simulation Property]
	Let $A$ be row $s$-sparse and row computable. Then $\e^{iAt_0}$ can be implemented up to error $\eps$ by a quantum circuit of size $O(\log N \cdot s^2 \cdot t_0)$ (where we have omitted factors of $t_0$ and $1/\eps$ that scale better than any power $\alpha > 0$, however small).

	Thus for row sparse matrices (\it{i.e.} $s = O(\poly n)$) the cirucit size is $O(\poly(\log N)\cdot t_0)$.
\end{theorem*}
\begin{proof}
	Beyond the scope of the course. \it{c.f.} D. Berry, G. Ahokas \it{et al.}.
\end{proof}
\begin{remark*}
	The requirements (i) and (ii) will not feature in the HHL algorithm itself; they are quoted here \it{only} to guarantee efficient implementability of the Hamiltonian Simulation for $\e^{iAt_0}$.
\end{remark*}

\begin{remark*}
	The matrices $A$ arising in finite element methods for numerical PDE solutions are row sparse \& row computable.
\end{remark*}

For the RHS vector $b$:


\begin{enumerate}
	\setcounter{enumi}{3}
	\item we will require that $b = (b_1,\dots,b_N)$ has $|b_1|^2 + \dots + |b_N|^2 = 1$ and that the ($\log N$)-qubit state $\sum b_i\ket{i}$ can be \it{efficiently} produced \it{i.e.} in time $\poly(n)$ on a quantum computer.
\end{enumerate}

\begin{remark*}
	If $b$ is not normalised, assume that $|b|^2 = \sum_{i=1}^{N}|b_i|^2$ can be efficiently computed, \& then we use the normalised version $\hat{b}$ of $b$. Finally at the end we rescale the output value using the already-computed $|b|^2$.
\end{remark*}

\begin{enumerate}
	\setcounter{enumi}{4}
	\item Require that $M$ is Hermitian \& the corresponding quantum measurement on $n = \log N$ qubits is implementable in $\poly(n)$ time on a quantum computer. If $m$ not Hermitian, $M = K + iL$, $K = (M+M^\dagger)/2$, $L = (M-M^\dagger)/2$, and then just do HHL for $K$ and $L$ separately.
\end{enumerate}

\marginpar{Lecture 13}

HHL will produce a state $\ket{\hat{x}} = ``x$ normalised'' to accuracy $\eps$ and then estimate $\mu = x^\dagger M x$, $M$ Hermitian, also to accuracy $\eps$ (by measurement of $M$ on it, mean value = $\mu$).

The best known classical general purpose algorithm to achieve the above runs in time $O(N\cdot s\sqrt{\kappa} \log(1/\eps))$.

HHL will output state within $\eps$ of $\ket{\hat{x}}$ in time $O(\log N \cdot s^2 \kappa^2 \cdot 1/\eps)$ and similar time to get $\mu$ to accuracy within $\eps$.

In the regime that $\eps = \frac{1}{\poly(\log N)}$ and all other conditions, the classical time is $O(\poly(N))$, and the HHL quantum time is $O(\poly(\log N))$, \it{an exponential speed-up}.

\begin{remark*}[The HHL Algorithm]
	For clarity, we first assume the ingredients PE, HamSim can be executed error-free, and we will \it{afterwards} give a discussion of errors and runtime.

	\begin{itemize}
		\item Assume $|\lambda_{\max}| = 1$ and condition number $\kappa$ is known (or an upper bound known), so all sizes of eigenvalues lie within $[1/\kappa,1]$.
		
		Work in $N$-dimensional space, $n = \log N$ qubits. Computational basis $\{\ket{i}:i = 0,\dots,N-1\}$.

		Let the eigenvectors and corresponding eigenvalues of $A$ be denoted $\lambda_j$ and $\ket{u_j}$ respectively, $j = 0,1,\dots,N-1$.

		Begin by implementing RHS state $\ket{b}$, and consider it in the eigenbasis $$\ket{b} = \sum_{i=0}^{N-1}b_i\ket{i} = \sum_{j=0}^{N-1}\beta_j\ket{u_j}$$ and then $$\ket{x} = A^{-1}\ket{b} = \sum_{j=0}^{N-1}\beta_j \frac{1}{\lambda_j}\ket{u_j}$$ - note here that $\ket{x}$ is not necessarily normalised here.

		This transformation is linear, but not unitary so cannot be implemented directly as a quantum operation. To instead achieve it probabilistically, we'll use PE for $U = \e^{iA}$ with the latter exponential (\& needed powers of C-U in PE) being implemented by HamSim. Then a so-called post-selected process (\it{cf} Sheet 1 Q6) will achieve our desired non-unitary transformation:

		\item PE with HamSim (all assumed to work perfectly) give:
		\begin{align*}
			\ket{b}\ket{0}\xrightarrow{U_{\textrm{PE}}} \sum \beta_j \ket{u_j}\ket{\lambda_j}
		\end{align*}
		which we denote $(\ast)$. Here we do \it{not} do final measurement in PE, only the unitary part.

		\item Next adjoin an extra ancilla \it{qubit} $\ket{0}$, and apply the following controlled rotation C-rotn on it, controlled by $\ket{\lambda_j}$ register.
		
		C-rot:
		\begin{align*}
			\ket{\lambda_j}\ket{0} & \mapsto \ket{\lambda_j} [\cos \theta_j \ket{0} + \sin\theta_j \ket{1}]\\
			&= \sqrt{1 - \frac{c^2}{\lambda_j^2}}\ket{\lambda_j}\ket{0} + \frac{c}{\lambda_j}\ket{\lambda_j}\ket{1}
		\end{align*}
		Here:
		\begin{itemize}
			\item $c$ is chosen to have $c\le \min_j |\lambda_j|$ (so we take $c = \frac{1}{\kappa}$ for definiteness)
			\item the angle of rotation $\theta_j \in (-\pi/2,\pi/2)$ given by $$\theta_j = \arcsin \frac{c}{\lambda_j}$$ determined by content of $\ket{\lambda_j}$ register.
		\end{itemize}
		Note: C-rot is a \it{fixed} operation on $n+1$ qubits that in fact can be implemented by a $\poly(n)$ sized circuit of $1$ and $2$-qubit gates (see Sheet 3 Q3) (for any $\lambda$ with $|\lambda < \eps|$).

		Apply C-rot to $(\ast)$ results in the state:

		\begin{align*}
			\ket{\psi_0} = \sum_{j=0}^{N-1}\beta_j\sqrt{1 - \frac{c^2}{\lambda_j^2}} \ket{u_j}\ket{\lambda_j}\ket{0} + \beta_j \frac{c}{\lambda_j}\ket{u_j}\ket{\lambda_j}\ket{1}
		\end{align*}
		from which we \it{want} the part associated to the ancilla state $\ket{1}$.

		\item (Post selection step) we measure the ancilla in the computational basis, hoping to get result 1. What is the probability that this happens? We will see result 1 with prob $p$ given by:
		
		\begin{align*}
			p &= \left| \left| \sum_{j}\beta_j \frac{c}{\lambda_j}\ket{u_j}\ket{\lambda_j}\right|\right|^2 = \sum_{j}\left|\beta_j\frac{c}{\lambda_j}\right|^2\\
			&= \frac{1}{\kappa^2}\sum_j \left| \frac{\beta_j}{\lambda_j}\right|^2 \ge \frac{1}{\kappa^2}
		\end{align*}
		where the last inequality comes from the fact that $\sum |\beta_j^2| = 1$ and $\frac{1}{\lambda_j} \ge 1$ since $|\lambda_j| \in [1/\kappa,1]$.

		In that case, the post-measurement state will be:

		\begin{align*}
			\ket{\Upsilon} &= \frac{1}{\sqrt{p}}c\sum_{j=0}^{N-1}\frac{\beta_j}{\lambda_j}\ket{u_j}\ket{\lambda_j}
		\end{align*}
		To mitigate the probability $p$, and obtain $\ket{\Upsilon}$ with any fixed (high) level of probabilty $1 - \eta$ ($\eta > 0$ small) we can repeat the whole process $\log(1/\eta)/p = O(\kappa^2)$ times, and outcome 1 will occur at least once with probability greater than $1 - \eta$. [This follow from the basic probability lemma: If a single trial has success probability $p$ and we repeat the trial $M$ times independently, then for any $0 < 1 - \eta < 1$, the probability of at least one succesful trial in $M$ exceeds $1-\eta$ if $M \ge -\log(\eta)/p$.]

		\begin{remark*}[Remark (Amplitude Amplification)]
			The $O(\kappa^2)$ repetitions above can be improved to $O(\kappa)$ bu use of AA, instead of repeated measurements.
		\end{remark*}

		\item Having obtained the state:
		\begin{align*}
			\ket{\Upsilon} &= \frac{1}{\sqrt{p}}c\sum_{j=0}^{N-1}\frac{\beta_j}{\lambda_j}\ket{u_j}\ket{\lambda_j}
		\end{align*}
		we run PE in reverse to ``uncompute'' or ``erase'' the $\ket{\lambda_j}$ register, resetting it to $\ket{0}$ (and discarding it) and obtain:

		\begin{align*}
			\ket{\hat{x}} &= \frac{c}{\sqrt{p}}\sum_{j=0}^{N-1}\frac{\beta_j}{\lambda_j}\ket{u_j} = \frac{1}{\sqrt{p}}\ket{x}
		\end{align*}
		\it{i.e.} normalised version of $\ket{x}$.

		\item Finally, we perform a measurement of observable $M$ on (copies of) $\ket{\hat{x}}$ [the copies are obtained by just running the whole process multiple times] to estimate its mean value, $$\frac{c^2}{p}\langle x | M | x \rangle = \frac{c^2}{p}\mu$$ via empirical mean of its values.
		
		According to the Chernoff-Hoeffding bound (see notes), $O(\log(1/\eta)/\xi^2)$ measurements of $M$ will suffice to estimate the mean $\mu c^2/p$ to any desired accuracy $\xi$, with any desired success probability $1 - \eta$.

		We know $c$. For $p$, we can similarly estimate the probability $p$ (in the post selection step) to any accuracy $\xi$ by applying the Chernoff-Hoeffding bound to the ancilla measurement outcome, 0 or 1, its mean value being $0p_0 + 1p_1 = p$, our $p$ that we want. Using these estimated values of $c^2/p \langle x|M x\rangle$ and $p$ (and $c = \frac{1}{\kappa}$) we obtain our value of $\mu = \langle x | M | x\rangle$.
	\end{itemize}
\end{remark*}

\begin{remark*}[Remarks about run time \& approximation errors]
	[Non-examinable; messy to do rigorously. For a full analysis: original HHL paper `Phys. Rev. Lett. vol 103, 150502 (2009)'.]

	\begin{itemize}
		\item we assume we can make the RHS vector state $\ket{b}$ exactly
		\item similarly the controlled rotation C-rot is a fixed unitary \& can be implemented efficiently - we assume it is exact too (ES3 Q3)
		\item Consider now the \underline{phase estimation} required. The PE algorithm for a unitary $U$ \& $n$ qubit lines gives estimate of $\lambda$ up to $n$ bits of precision. We denote it by $\lambda'$ and assume for now that it is the closest $n$-bit approximation to $\lambda$. \it{I.e.} we get $\lambda$ up to additive error $\eta  = 1/2^n$. The PE process uses Hamiltonian Simulation that requires execution of controlled $U,U^2,\dots,U^{2^{n-1}}$. If $U = \e^{iA}$, then $C-U = \e^{i\tilde{A}}$, where
		\begin{align*}
			\tilde{A} = \left[\begin{array}{ccccccc}
				0 & \dots & 0 & \vline & & & \\
				&&&\vline & &0 &\\
				0 & \dots & 0 & \vline & &&\\
				\hline & 0 & &\vline & & A & \end{array}\right]
		\end{align*}
		is double the dimension of $A$, and is $s$-sparse/row-computable if $A$ was. note that $C-U = \left[\begin{array}{ccc} I & \vline & 0 \\ \hline 0 &\vline & U\end{array}\right]$. Thus we need to implement $\e^{i\tilde{A}t}$ for $t = 1,2,4,\dots,2^{n-1}$, which is done using HamSim for $s$-sparse matrices with circuit sizes $O(\log N \cdot t \cdot s^2)$, so total circuit size is $O(\log N \cdot t_0 \cdot s^2)$, with $t_0 = 1 + 2 + \dots + 2^{n-1} \approx 2^n = 1/\eta$.

		After the controlled rotation C-rot using the $\lambda'$s produced and measurement in post-selection step, we denote the final state produced by:

		\begin{align*}
			\ket{\hat{x}'} &= \frac{1}{D'} \sum \beta_j\frac{c}{\lambda_j'}\ket{u_j}\\
			\textrm{where }D' &= \sqrt{p'} = \sqrt{\sum_{j}|\beta_j|^2 \frac{c^2}{\lambda_j'^2}}
		\end{align*}
		and we write $\ket{\hat{x}},X,p$ for the corresponding expressions with (unexact) $\lambda'$s replaced by the true value $\lambda$s.

		Our requirement is that $\ket{\hat{x}}$ be within $\eps$ of the eaxct state $\ket{\hat{x}}$ and need to choose a correspondingly suitably large $t_0 = \frac{1}{\eta}$ \it{i.e.} suitably good accuracy of $\lambda'$s vs. $\lambda$s.

		To establish dependence of $t_0$ (or accuracy $\eta = 1/t_0$ in $\lambda$) on a desired final error tolerance $\eps$, we use two facts:

		\begin{enumerate}[label = (\alph*)]
			\item If $\lambda$ has additive error $\delta(\lambda) = \eta$ then $\frac{1}{\lambda}$ has $\delta(1/\lambda) \sim \frac{1}{\lambda^2}\delta(\lambda)$ so $1/\lambda$ has relative error $\delta(1/\lambda)/(1/\lambda) = \eta/\lambda$.
			
			\item If $A',B'$ approximate $A,B$ to relative error $\xi$ then $A'/B'$ approximates $A/B$ to relative error $\xi$ too:
			\begin{align*}
				\frac{A'}{B'} = \frac{A(1 + O(\xi))}{B(1 + O(\xi))} = \frac{A}{B}(1 + O(\xi))
			\end{align*}
		\end{enumerate}
		%
		Now by (a), $1/\lambda_j'$ approximates $1/\lambda_j$ to relative error $\delta(1/\lambda)/(1/\lambda) = O(\eta/\lambda_j) = O(\kappa\eta)$ as $1/\lambda_j \le \kappa$.

		Also similarly $D'$ approximates $D$ to relative error $O(\kappa\eta)$ too as $D$ is homogeneous of degree 1 in in $1/\lambda_j'$s. Then by (b) with $A' = \frac{\beta_j c}{\lambda_j'}$ and $B' = D'$ we see that amplitudes of $\ket{\hat{x}'}$ approximate those of $\ket{\hat{x}}$ up to multiplicative error $O(\kappa\eta)$ so:
		\begin{align*}
			||\ket{\hat{x}'} - \ket{\hat{x}}|| &= ||(1 + O(\kappa\eta))\ket{\hat{x}} - \ket{\hat{x}}||\\
			&= O(\kappa\eta) \textrm{ (want $\eps$)}
		\end{align*}

		Thus for given $\eps$ we choose $\kappa\eta = \eps$ \it{i.e.} $\eta = \eps/\kappa$, $t_0 = 1/\eta$ so we get $t_0 = \kappa/\eps$, and the PE step (run once) has run time $O(\log N tt_0 s^2) = O(\log N (\kappa/\eps)s^2)$.

		In HHL algorithm, to obtain $\hat{x}$ with high probability we use amplitude amplification which requires repeating the PE process $O(\kappa)$ times.

		So the final total run time is $O(\log N \cdot \kappa^2 s^2 \eps^{-1})$ \it{i.e.} $O(\poly(\log N),\eps^{-1})$ for $s,\kappa = O(\poly(\log N))$.
	\end{itemize}
\end{remark*}

\begin{remark*}[Further comments on error \& run time]\ 
	\begin{itemize}
		\item In above, have assumed the PE process (with $n$ qubit lines) outputs the closest $n$-bit approximation $\lambda'$ to $\lambda$, whereas in fact it outputs a superposition of all $\lambda_k = \lambda' \pm k/2^n$ (peaked around $k = 0$), which introduces a further source of error that has so far been completely ignored.
		
		To mitigate this, we use Thm. PE(b) \it{i.e.} use more qubit lines in PE, to get the best $n$-bit $\lambda'$ with high probability.

		Analysis is omitted; see original paper for details. But can show that this does not increase the above complexity/runtime limit of the HHL algorithm.

		In HHL paper, an ``improved PE algorithm'' is used, with a non-uniform initial superposition on qubit lines, giving a better concentration around the best $\lambda'$ above.

		\item In the HHL algorithm, the PE step is the source of the worst accuracy ($\eps$)-dependence being $O(1/\eps)$. This was subsequently improved (in 2013) in D. Berry, A. Childs, R. Cleve, ...., R. Somma arxiv: 1312.1414, improving $O(1/\eps)$ exponentially to $O(\log(1/\eps))$. The technieque involves replacing the PE process by a different method altogether, the so-called method of ``linear combination of unitaries''.
		
		See ES3 Q5 for basic idea involved.

		It is in fact quite a general method, and can be used to improve the $\eps$-dependence of HamSim too, again in a dramatic, exponential way.
	\end{itemize}
\end{remark*}

\subsection*{Clifford Computations \& Classical Simulation of Quantum Computing}

\underline{Question}: What is the ``key quantum effect or resource'' giving quantum computing its benefit over classical computing?

It's worth noting that it is currently in fact unproven whether QC is in fact faster than CC; \it{c.f.} P vs NP. But as things stand it is faster.

To formalise the comparison of classical vs quantum computing we introduce the precise mathematical notion of \undf{classical simulation of quantum computation}:
\begin{itemize}
	\item description of a quantum circuit $C$ \it{i.e.} list of $1$ and $2$-aubit gates on $n$ qubits, of size $N = \poly(n)$
	\item description of input, say $\ket{i_1\dots i_n}$ or $\ket{\alpha_1}\dots\ket{\alpha_n}$ - general \it{product} state
	\item nominated output line(s) [say just one (1st line) for yes/no answer]
\end{itemize}
Note that the full description is of size $\poly(n)$ classically.

\underline{Want}: by classical means only:

\textbf{Weak simulation}: sample the output distribution (using classical computer, that can make classical random choices too - \it{e.g.} ``toss a fair coin'' will suffice)

\textbf{Strong simulation}: \it{calculate} output probabilities $p_0$ and $p_1$ [see ES3 for multiline case]

Moreover, we want to do it \it{efficiently} \it{i.e.} in \it{classical} $\poly(n)$ time \it{i.e.} ``efficient classical simulation''.

In that case: we would have no quantum computational benefits over classical (up to some polynomial-time overhead, which we don't really care about).

\underline{Note}: the quantum process itself gives only a weak simultion (``self-simulation''), \it{i.e.} sample of output distribution. So a strong simulation is asking for \underline{much} more.

A ``direct'' strong classical simulation is always possible, but not generally efficient: the action of successive gates is just linear algebra, and clearly can calculate all evolving amplitudes \it{etc.}, \underline{but}: linear algebra develops in $2^n$-dimensional space, so it takes exponential time to do matrix multiplications \it{etc.}

However: \it{if} it is promised that the state is a pdouct state $\ket{\alpha_1}\dots\ket{\alpha_n}$ at \it{every} stage, then we get an \it{efficient} strong simulation, since each step involves one or two qubits in a known state \it{eg.} $\ket{\alpha_i},\ket{\alpha_j}$ separate from all others.

\begin{remark*}[Example]
	2 qubit gate $U$ on qubits 1 \& 2 of $$\ket{\psi} = \sum c_{i_1\dots i_n}\ket{i_1\dots i_n}$$ with amplitudes of $U\ket{\psi}\sim \tilde{c}_{i_1\dots i_n}$ given by $$\tilde{c}_{i_1\dots i_n} = \underbrace{\sum_{k_1,k_2=0}^{1} U_{i_1i_2}^{k_2 k_2}}_{\textrm{$4\times4$ product}}c_{k_1k_2\underbrace{i_3\dots i_n}_{\textrm{exp. choices}}}$$
	But if $\ket{\psi}$ a product, then $c_{i_1\dots i_n} = a_{i_1}b_{i_2}\dots d_{i_n}$ factorises and $$\tilde{c}_{i_1\dots i_n} = \left(\sum_{0}^{1}U^{k_1k_2}_{i_1i_2}a_{k_1}b_{k_2}\right)c_{i_3}\dots d_{i_n}$$, so just update the 2 qubits and factorise the answer back into $\tilde{a}_{i_1}\tilde{b}_{i_2}$ (by product state promise).
\end{remark*}

Note also that ``product state'' = ``abscence of entanglement'', and it is sometimes thus claimed that entanglement is the key resource for quantum computational benefits over classical. BUT: the above shows only that no entanglement $\implies$ no qcomp benefit, but it says nothing of the converse. So entanglement is \underline{necessary}, but \it{not} sufficient and there is more to do here.

Notably, consider:

\subsubsection*{Clifford Computation}

\textbf{Preliminary Definitions}:

The \undf{$n$-qubit Pauli group} $\mathcal{P}_n = \{\pm1,\pm i p_1\otimes\dots \otimes P_n : \textrm{ each }P_i\textrm{ is }I,X,Y,Z\}$ is a finite subgroup of $U(2^n)$.

A \undf{Clifford operation} $C$ on $n$ qubits is any unitary that \underline{conjugates} Paulis to Paulis. \it{i.e.} for all $P \in \mathcal{P}_n\ CPC^\dagger = \tilde{P}$ is in $\mathcal{P}_n$ too. The collection of all such operations forms the \undf{Clifford group}, which is the normaliser group of the subgroup $\mathcal{P}_n$ in $U(2^n)$.

These two groups are important in many applications:
\begin{enumerate}[label = \arabic*)]
	\item theory of quantum error correction (``stabiliser codes'') \& fault-tolerant quantum computation
	\item insights into quantum computing power
	\item have very elegant mathematical properties
\end{enumerate}

\begin{remark*}[Examples]\ 
	\begin{enumerate}
		\item all Paulis are Cliffords (clearly)
		\item $H$ is Clifford
		\item $S = \left(\begin{array}{cc} 1 & 0\\ 0 & i\end{array}\right) = \pi/2$ phase gate is Clifford
		\item $CX$ (2 qubit gate) is Clifford
		\item also have $Z = S^2, X = HZH$ and so $\textrm{SWAP}_{12} = CX_{12}CX_{21}CX_{12}, CZ_{12} = H_x(CX)_{12}H_2$ are Cliffords
	\end{enumerate}
\end{remark*}

In fact there is a full explicit characterisation of Cliffords (proof omitted):

\begin{theorem*}
	$C$ on $n$ qubits is Clifford if and only if $C$ is a cirucit of $H,S$ and $CX$ gates.
\end{theorem*}

\undf{Clifford computations}: circuits that involve only Clifford gates. They have very special classical simulation properties (\it{cf.} below).

\underline{Note}: we can generate much entanglement using Clifford operations. \it{eg.} $$\ket{0}_1\ket{0}_2 \xrightarrow{H_1} (\ket{0} + \ket{1})\ket{0} \xrightarrow{CX_{12}}\ket{0}\ket{0} + \ket{1}\ket{1}$$ and similarly we can make $$\frac{1}{\sqrt{2}}(\ket{0\dots 0} + \ket{1\dots 1})$$ the ``cat state''.

\begin{theorem*}[Gottesman-Knill Theorem (variant)]
	Let $C$ be any Clifford circuit on $n$ qubits, size $N = \poly(n)$ (say given as a cirucit of $H,S$ and $CX$ gates) with input any product state $\ket{a_1}\dots\ket{a_n}$ and output a measurement on the first qubit line (or any other line).

	Then the output can always be classically strongly efficiently simulated (ans so weakly too).
\end{theorem*}

There are many different proofs of this using a variety of techniques - we will give one here.

\begin{remark*}
	An alternative proof uses the so-called \it{stabiliser formalism} (introduced for quantum error correction theory) and this was the origin of the GK Theorem. See \it{e.g.} Nielsen \& Chuang Ch. 10.5.
\end{remark*}
\marginpar{Lecture 16}
\begin{proof}
	Write $Z_1 = Z\otimes I \otimes \dots \otimes I$. Recall $Z = \ket{0}\bra{0} - \ket{1}\bra{1}$.

	So $Z_1 = \Pi_0 - \Pi_1$ with $\Pi_{0,1}$ being $n$-qubit projections to $1^{\textrm{st}}$ qubit being $0,1$ respectively.

	So $p_0 - p_1 = \langle \psi_{\textrm{final}}|z_1|\psi_{\textrm{final}}\rangle = \langle \psi_{\textrm{in}}|C^\dagger Z_1C|\psi_{\textrm{in}}$ since $\ket{\psi_{\textrm{final}}} = C\ket{\psi_{\textrm{in}}}$. Now if $C = C_N\dots C_2C_1$ (each $C_i$ is $H,S,CX$) we get $p_0 - p_1 = \langle \psi_{\textrm{in}}|C_1^\dagger \dots C_N^\dagger Z_1 C_N\dots C_1 | \psi_{\textrm{in}}\rangle$. By the defining property of Cliffords, we finally get that $$p_0 - p_1 = \langle \psi_{\textrm{in}}|\tilde{P}_1\otimes\dots\otimes\tilde{P}_n|\psi_{\textrm{in}}\rangle\quad (\ast)$$ where each $\tilde{P}$ is a Pauli.

	All update rules for Pauli products by $H,S,CX$ conjugations are easy to compute (classically) and record once \& for all. [Indeed, $H,S,CX$ are only $1,2$-qubit gates, so only constant size calculation is required.]

	Finally we use $\ket{\psi_{\textrm{in}}} = \ket{a_1}\dots\ket{a_n}$ is \it{product} state, so $$p_0 - p_ = \prod_{i=1}^{n}\langle a_i | \tilde{P}_i | a_i\rangle$$ [$(\ast)$ is a \underline{product} operator wedged between \underline{product} states.] So $p_0 - p_1$ is a product of $n$ $2\times 2$ matrix calculations, so this is a poly-time (in fact, linear time) calculation and is efficiently computable.

	Then since $p_0 + p_1 = 1$, we get $p_0$ and $p_1$ with efficient classical computation.
\end{proof}

Now let's extend (unitary) Clifford circuits to allow inclusion of intermediate measurement ($1$-qubit) and carry on using post measurement state. We distinguish two scenarios:

\begin{itemize}
	\item \undf{Adaptive}: Choice of later gates \it{can} depend on the outcome of earlier measurements
	\item \undf{Non-adaptive}: Choice of later gates \it{cannot} depend on the outcome of earlier measurements.
\end{itemize}

\begin{theorem*}
	Let $C$ be any Clifford circuit with intermediate measurements, arbitrary product state inputs, and single line outputs.
	\begin{enumerate}[label=(\alph*)]
		\item if $C$ is \emph{non-adpative} then the output is classically efficiently strongly simulatable
		\item if $C$ is \emph{adaptive} then full universal quantum computation is possible
	\end{enumerate}
\end{theorem*}
\begin{remark*}
	Can show as well that if the input is only \it{computatinal basis} states, then (b) remains classically efficiently simultable but only weakly (and strongly if P = NP, but we won't worry about that).

	For this and more variants: R. Jozsa \& M. van der Nest arxiv: 1305.6190.
\end{remark*}

\begin{proof}
	[Ideas.] For (a), just note that in fact non-adaptive Clifford circuits are easily reducible to the fully unitary case (\& then use GK Thm). In fact (ES3) the following can be seen to be equivalent:

	Scenario A: $C_1$, then intermediate measurement, then $C_2$, where $C_2$ is independent of measurement outcome.

	Scenario B: $C_1$ then $C_2$, but introduce extra line with input $\ket{0}$ ancilla, and instead of measurement do a Clifford CX with the ancilla and then discard the ancilla.
	
	The reason for the equivalence is that the CX separates the $\ket{0}$ and $\ket{1}$ that would be measured in Scenario A into $\ket{0}\ket{0}$ and $\ket{1}\ket{1}$ orthogonal states, with the same probabilistic weightings. Then they are completely separated and it is as though they have been measured.

	Note this is only requires us to add in a polynomial amount of stuff, so the computation remains efficient.

	For (b): Use Bravyi-Kitaev idea of so-called \undf{magic states}, together with the following preliminary fact:

	\underline{Fact}: Clifford gates with the $1$-qubit (non-Clifford) $T$ gate, $T = \left(\begin{array}{cc}1 & 0\\0 & \e^{i\pi/4}\end{array}\right) = \sqrt{S} = $ Phase($\pi/4$) gate, is universal for quantum computing.

	The magic states are going to be used to implement a $T$-gate, but without actually using a $T$-gate.

	let $\ket{A}$ denote the $1$-qubit state $$\ket{A} = \frac{1}{\sqrt{2}}\left(\ket{0} + \e^{i\pi/4}\ket{1}\right)$$ a so-called magic state. We will implement a $T$ gate (on any line $k$) using a $1$-qubit ancilla $\ket{A}$ and an adaptive Clifford circuit, known as the \undf{T-gadget}:

	We have $\ket{\psi}$ as our input state across $n$ lines, and we focus on line $k$ and add ancilla $\ket{A}$. Perform a controlled not on line $k$ with $\ket{A}$, and measure the ancilla to obtain result $m = 0$ or $1$. Then on line $k$ apply $S^m$, which is either $I$ or $S$ depending on the outcome $m$, and then discard the ancilla.

	The claim here is that we then always get $T_k\ket{\psi}$, a $T$ gate acting on $\ket{\psi}$ on line $k$, as the outcome of this process. Note also that the probabilities for $m =0,1$ are always $1/2,1/2$. See ES3 for further details.

	So we can make $T$-gates, and hence generate full universal quantum computing by the Fact.
\end{proof}

\textbf{Finale.}

We thus have: for Clifford circuits with product state inputs, single line outputs and intermediate measurements:
\begin{enumerate}[label=(\alph*)]
	\item non-adaptive - classically (strongly) efficiently simulatable
	\item adaptive - has full universal quantum computing power
\end{enumerate}
\it{i.e.} for Cliffords $C_k$, $M(i,y) \equiv$ ``measurement of line $i$, get result $y = 0$ or $1$'' we have:
\begin{enumerate}[label=(\alph*)]
	\item $C_0 M(i,y_1)C_1M(i_2,y_2)C_2\dots$
	\item $C_0M(i,y_1)C_1(y_1)M(i_2,y_2)C_2(y_1,y_2)\dots$
\end{enumerate}
Therefore a purely classical ingredient, \it{adaptive choices of gate} (note: measurements are done in \underline{both} scenarios) is a resource that elevates classically limited power to full quantum power.

Furthermore, experimentally there is ``no difference'' between (a) \& (b), in the sense that there are no new quantum processes in (b) that do not already occur in (a); a run of an adaptive circuit could have occurred as a run of a non-adaptive circuit with the same outcomes.

\end{document}
